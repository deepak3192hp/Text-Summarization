{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0dhLSL8Iv9w5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = '********'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"auto\",\n",
        "    api_key=os.environ[\"HF_TOKEN\"],\n",
        ")\n",
        "\n",
        "result = client.summarization(\n",
        "    \"The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct.\",\n",
        "    model=\"facebook/bart-large-cnn\",\n",
        ")"
      ],
      "metadata": {
        "id": "dsMCaetuzJzX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLfmKqgD1HbL",
        "outputId": "78ad43f0-ab55-4e37-cfac-303db23c353b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SummarizationOutput(summary_text='The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il17mHfNZ2Co",
        "outputId": "4f66979f-5aea-45f4-f79a-93dd9c4b630a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "# Open the PDF in binary mode\n",
        "reader = PdfReader(\"/content/sample_data/_OceanofPDF.com_learning_langchain_-_mayo_oshin.pdf\")\n",
        "\n",
        "# Initialize an empty string to store the text\n",
        "full_text = \"\"\n",
        "\n",
        "# Loop through all pages and extract the text\n",
        "for page in reader.pages:\n",
        "    full_text += page.extract_text()\n",
        "\n",
        "print(full_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2fuhWi8Ypxn",
        "outputId": "72434905-807b-4bf6-bc2b-196c037cdb18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning LangChain\n",
            "Build an AI Chatbot Trained on Y our Data\n",
            "With Early Release ebooks, you get books in their earliest\n",
            "form—the authors’ raw and unedited content as they write\n",
            "—so you can take advantage of these technologies long\n",
            "before the oﬀicial release of these titles.\n",
            "Mayo Oshin and Nuno Campos\n",
            "Learning LangChain\n",
            "by Mayo Oshin  and Nuno Campos\n",
            "Copyright © 2025 Olumayowa Olufemi Oshin. All rights\n",
            "reserved.\n",
            "Printed in the United States of America.\n",
            "Published by O’Reilly Media, Inc. , 1005 Gravenstein\n",
            "Highway North, Sebastopol, CA 95472.\n",
            "O’Reilly books may be purchased for educational, business,\n",
            "or sales promotional use. Online editions are also available\n",
            "for most titles ( http://oreilly .com ). For more information,\n",
            "contact our corporate/institutional sales department: 800-\n",
            "998-9938 or corporate@oreilly .com .\n",
            "Acquisitions Editor:  Nicole Butterﬁeld\n",
            "Development Editor:  Corbin Collins\n",
            "Production Editor:  Clare Laylock\n",
            "Interior Designer:  David Futato\n",
            "Cover Designer:  Karen Montgomery\n",
            "Illustrator:  Kate DulleaApril 2025:  First Edition\n",
            "Revision History for the Early\n",
            "Release\n",
            "2024-06-18:  First Release\n",
            "See http://oreilly .com/catalog/errata.csp?\n",
            "isbn=9781098167288  for release details.\n",
            "The O’Reilly logo is a registered trademark of O’Reilly\n",
            "Media, Inc. Learning LangChain , the cover image, and\n",
            "related trade dress are trademarks of O’Reilly Media, Inc.\n",
            "The views expressed in this work are those of the authors\n",
            "and do not represent the publisher’s views. While the\n",
            "publisher and the authors have used good faith eﬀorts to\n",
            "ensure that the information and instructions contained in\n",
            "this work are accurate, the publisher and the authors\n",
            "disclaim all responsibility for errors or omissions, including\n",
            "without limitation responsibility for damages resulting from\n",
            "the use of or reliance on this work. Use of the information\n",
            "and instructions contained in this work is at your own risk.\n",
            "If any code samples or other technology this work containsor describes is subject to open source licenses or the\n",
            "intellectual property rights of others, it is your\n",
            "responsibility to ensure that your use thereof complies with\n",
            "such licenses and/or rights.\n",
            "978-1-098-16722-6\n",
            "[LSI]Brief Table of Contents (Not Yet\n",
            "Final)\n",
            "Preface (available)\n",
            "Chapter 1: LLM Fundamentals with LangChain (available)\n",
            "Chapter 2: Indexing: Preparing Y our Documents for LLMs\n",
            "(available)\n",
            "Chapter 3: Retrieval: Chatting with Y our Documents\n",
            "(unavailable)\n",
            "Chapter 4: Memory: Making Y our Chatbot Remember and\n",
            "Learn from Interactions  (unavailable)\n",
            "Chapter 5: Agents: Getting Y our AI Chatbot Thinking and\n",
            "Acting  (unavailable)\n",
            "Chapter 6: Human-in-the-loop: Collaborating with LLMs\n",
            "(unavailable)\n",
            "Chapter 7: Release: Deploying Y our AI Chatbot\n",
            "(unavailable)Chapter 8: Maintenance: Monitoring and Continuous\n",
            "Improvement  (unavailable)Preface\n",
            "A NOTE FOR EARLY RELEASE READERS\n",
            "With Early Release ebooks, you get books in their earliest\n",
            "form—the author’s raw and unedited content as they write\n",
            "—so you can take advantage of these technologies long\n",
            "before the oﬀicial release of these titles.\n",
            "This will be the preface of the ﬁnal book. Please note that\n",
            "the GitHub repo will be made active later on.\n",
            "If you have comments about how we might improve the\n",
            "content and/or examples in this book, or if you notice\n",
            "missing material within this chapter , please reach out to\n",
            "the editor at ccollins@oreilly .com .\n",
            "On November 30, 2022, San Francisco-based research ﬁrm\n",
            "OpenAI publicly released  ChatGPT—the viral AI chatbot\n",
            "that can generate content, answer questions, and solve\n",
            "problems like a human. W ithin two months of its launch,\n",
            "ChatGPT attracted over 100 million  monthly active users,\n",
            "the fastest adoption rate of a new consumer technology\n",
            "application (so far). ChatGPT is a chatbot experiencepowered by an instruction and dialogue-tuned version of\n",
            "OpenAI’s GPT -3.5 family of large language models (LLMs).\n",
            "We’ll get to deﬁnitions of these concepts very shortly .\n",
            "NOTE\n",
            "Building LLM applications with or without LangChain requires the use\n",
            "of an LLM (read on for explanations for all these concepts). In this book\n",
            "we’ll be making use of the OpenAI API, whose pricing can be found\n",
            "here, as the LLM provider we use in the code examples. One of the\n",
            "beneﬁts of working with LangChain (more on this later) is you can\n",
            "follow all along all of these examples using either OpenAI or alternative\n",
            "commercial or open source LLM providers. We’ll call this out\n",
            "throughout the book.\n",
            "Three months later , OpenAI released  the ChatGPT API,\n",
            "giving developers access to the chat and speech-to-text\n",
            "capabilities. This kickstarted an uncountable number of\n",
            "new applications and technical developments under the\n",
            "loose umbrella term of generative AI .\n",
            "Before we deﬁne generative AI and LLMs, let’s think back\n",
            "to the concept of machine learning (ML). Some computer\n",
            "algorithms  (think: repeatable recipes for achievement of\n",
            "some predeﬁned task, such as sorting a deck of cards) are\n",
            "directly written by a software engineer . Other computeralgorithms are instead learned  from vast amounts of\n",
            "training examples—the job of the software engineer shifts\n",
            "from writing the algorithm itself to writing the training\n",
            "logic that creates the algorithm. A lot of attention in the ML\n",
            "ﬁeld went into developing algorithms for predicting any\n",
            "number of things, from tomorrow’s weather to the most\n",
            "eﬀicient delivery route for an Amazon driver .\n",
            "With the advent of LLMs and other generative models (like\n",
            "diﬀusion models for generating images, which we don’t\n",
            "cover in this book), those same ML techniques are now\n",
            "applied to the problem of generating new content, such as\n",
            "a new paragraph of text or drawing, that is at the same\n",
            "time unique and informed by examples in the training data.\n",
            "LLMs in particular are generative models dedicated to\n",
            "generating text.\n",
            "LLMs have two other diﬀerences from previous ML\n",
            "algorithms:\n",
            "1. They are trained on much larger amounts of data\n",
            "Training one of these models from scratch would be\n",
            "very costly).\n",
            "2. They are more versatileThe same text generation model can be used for\n",
            "summarization, translation, classiﬁcation, and so\n",
            "forth, whereas previous ML models were usually\n",
            "trained and used for a speciﬁc task.\n",
            "These two diﬀerences (which are maybe related to each\n",
            "other , but that’s outside the scope of this book) conspire to\n",
            "make the job of the software engineer shift once more, with\n",
            "increasing amounts of time dedicated to working out how\n",
            "to get an LLM to work for their use case. And that’s what\n",
            "this book (and LangChain) is all about.\n",
            "By the end of 2023, competing LLMs emerged, including\n",
            "Anthropic’s Claude and Google’s Bard, providing even\n",
            "wider access to these new capabilities. And subsequently ,\n",
            "thousands of successful startups and major enterprises\n",
            "have incorporated generative AI APIs to build applications\n",
            "for various use cases, ranging from customer support\n",
            "chatbots to writing and debugging code.\n",
            "On October 22, 2022, Harrison Chase published the ﬁrst\n",
            "commit  on GitHub for the LangChain open source library .\n",
            "LangChain started from the realization that the most\n",
            "interesting LLM applications needed to use LLMs together\n",
            "with “other sources of knowledge and computation” . Forinstance, you can try to get an LLM to generate the answer\n",
            "to this question:\n",
            "You’ll likely be disappointed by its math prowess. However ,\n",
            "if you pair it up with a calculator function, you can instead\n",
            "instruct the LLM to reword that question into an input a\n",
            "calculator could handle:\n",
            " 1,234 % 123\n",
            "Then you can pass that to a calculator function and get an\n",
            "accurate answer to your original question. LangChain was\n",
            "the ﬁrst (and is still today the largest) library to provide\n",
            "such building blocks and the tooling to reliably combine\n",
            "them into larger applications. Before looking deeper into\n",
            "what it takes to build compelling applications with these\n",
            "new tools, let’s brieﬂy look at what LLMs and LangChain\n",
            "are.\n",
            "Brief Primer on LLMsHow many balls are left after splitting 1,234 balIn layman’s terms, LLMs are trained algorithms that\n",
            "receive text input and predict and generate human-like text\n",
            "output. Essentially , they behave like the familiar word\n",
            "autocomplete feature found on many smartphones, but\n",
            "taken to an extreme.\n",
            "Let’s break down the term large language model:\n",
            "Language model  refers to a computer algorithm trained\n",
            "to receive written text (in English or other languages)\n",
            "and produce output also as written text (in the same\n",
            "language, or a diﬀerent one). These are neural networks ,\n",
            "a type of ML model which resembles a stylized\n",
            "conception of the human brain, with the ﬁnal output\n",
            "resulting from the combination of the individual outputs\n",
            "of many simple mathematical functions, called neurons ,\n",
            "and their interconnections. If many of these neurons are\n",
            "organized in speciﬁc ways, with the right training\n",
            "process and the right training data, this produces a\n",
            "model that is capable of understanding the meaning of\n",
            "individual words and sentences, which makes it possible\n",
            "to use them for generating plausible, readable, written\n",
            "text.\n",
            "Large  refers to the size of these models in terms of\n",
            "training data and parameters used during the learningprocess. F or example, OpenAI’s GPT -3 model contains\n",
            "175 billion parameters , which were learned from training\n",
            "on 45 terabytes of text data. Parameters  in a neural\n",
            "network model are made up of the numbers that control\n",
            "the output of each neuron  and the relative weight of its\n",
            "connections with its neighboring neurons. (Exactly which\n",
            "neurons are connected to which other neurons varies for\n",
            "each neural network architecture, and is beyond the\n",
            "scope of this book.)\n",
            "Because of the prevalence of English in the training data,\n",
            "most models are better at English than they are at other\n",
            "languages with a smaller number of speakers. By better  we\n",
            "mean it is easier to get them to produce desired outputs in\n",
            "English. There are LLMs designed for multilingual output,\n",
            "such as BLOOM , that use a larger proportion of training\n",
            "data in other languages. Curiously , the diﬀerence in\n",
            "performance between languages isn’t as large as might be\n",
            "expected, even in LLMs trained on a predominantly English\n",
            "training corpus, with researchers  having found that LLMs\n",
            "are able to transfer some of their semantic understanding\n",
            "to other languages.\n",
            "Put together , large language models are instances of big,\n",
            "general-purpose language models that are trained on vastamounts of text. In other words, these models have learned\n",
            "from patterns in large datasets of text—books, articles,\n",
            "forums, and other publicly available sources—to perform\n",
            "general text-related tasks. These tasks include text\n",
            "generation, summarization, translation, classiﬁcation, and\n",
            "more.\n",
            "Let’s say we instruct an LLM to complete the following\n",
            "sentence:\n",
            "The capital of England is  _______.\n",
            "The LLM will take that input text and predict the correct\n",
            "output answer as London. This looks like magic, but it’s\n",
            "not. Under the hood, the LLM estimates the probability of a\n",
            "sequence of word(s) given a previous sequence of words.TIP\n",
            "Technically speaking, the model makes predictions based on tokens,\n",
            "not words. A token represents an atomic unit of text. Tokens can\n",
            "represent individual characters, words, subwords, or even larger\n",
            "linguistic units, depending on the speciﬁc tokenization approach used.\n",
            "For example, using GPT-3.5’s tokenizer (called cl100k), the phrase good\n",
            "morning dearest friend would consist of ﬁve tokens (using _ to show the\n",
            "space character):\n",
            "Good: With token ID 19045\n",
            "_morning: With token ID 6693\n",
            "_de: With token ID 409\n",
            "arest: With token ID 15795\n",
            "_friend: With token ID 4333\n",
            "Usually tokenizers are trained with the objective of having the most\n",
            "common words encoded into a single token, for example, the word\n",
            "morning is encoded as the token 6693. Less common words, or words\n",
            "in other languages (usually tokenizers are trained on English text),\n",
            "require several tokens to encode them. For example, the word dearest\n",
            "is encoded as tokens 409, 15795. One token is approximately four\n",
            "characters of text for common English text, or roughly three quarters\n",
            "of a word.\n",
            "The driving engine behind LLMs’ predictive power is\n",
            "known as the transformer neural network architecture . The\n",
            "transformer architecture enables models to handlesequences of data like sentences or lines of code and make\n",
            "predictions about the likeliest next word(s) in the sequence.\n",
            "Transformers are designed to understand the context of\n",
            "each word in a sentence by considering it in relation to\n",
            "every other word. This allows the model to build a\n",
            "comprehensive understanding of the meaning of a\n",
            "sentence, paragraph, and so on (in other words, a sequence\n",
            "of words) as the joint meaning of its parts in relation to\n",
            "each other .\n",
            "So, when the model sees the sequence of words  the capital\n",
            "of England is , it makes a prediction based on similar\n",
            "examples it saw during its training. In the model’s training\n",
            "corpus the word England (or the token(s) that represent it)\n",
            "would have often shown up in sentences in similar places to\n",
            "words like France, United States, China. The word capital\n",
            "would ﬁgure in the training data in many sentences also\n",
            "containing words like England, France, and US , and words\n",
            "like London, P aris, W ashington. This repetition during the\n",
            "model’s training resulted in the capacity to correctly\n",
            "predict that the next word in the sequence should be\n",
            "London.\n",
            "The instructions and input text you provide to the model is\n",
            "called a prompt . Prompting can have a signiﬁcant impacton the quality of output from the LLM. There are several\n",
            "best practices for prompt design  or prompt engineering ,\n",
            "including providing clear and concise instructions with\n",
            "contextual examples, which we discuss later in this book.\n",
            "Before we go further into prompting, let’s look at some\n",
            "diﬀerent types of LLMs available for you to use.\n",
            "The base type, from which all the others derive, is\n",
            "commonly known as a pre-trained LLM : it has been trained\n",
            "on very large amounts of text (found on the internet, and in\n",
            "books, newspapers, code, video transcripts, and so forth) in\n",
            "a self -supervised fashion. This means that—unlike in\n",
            "supervised ML, where prior to training the researcher\n",
            "needs to assemble a dataset of pairs of input  to expected\n",
            "output —for LLMs those pairs are inferred from the training\n",
            "data. Y ou might ask why , and the sheer size of the training\n",
            "data is the best answer . The only way to use datasets that\n",
            "are so large is to assemble those pairs from the training\n",
            "data automatically . Two techniques to do this involve\n",
            "having the model do the following:\n",
            "Predict the next word\n",
            "Remove the last word from each sentence in the\n",
            "training data, and that yields a pair of input  andexpected output , such as The capital of England is ___\n",
            "and London .\n",
            "Predict a missing word\n",
            "Similarly , if you take each sentence and omit a word\n",
            "from the middle, you now have other pairs of input\n",
            "and expected output, such as The ___ of England is\n",
            "London  and capital .\n",
            "These models are quite diﬀicult to use as-is. Y ou need to\n",
            "prime the response with a suitable preﬁx. F or instance, if\n",
            "you want to know the capital of England, you might get a\n",
            "response by prompting the model with The capital of\n",
            "England is .\n",
            "Instruction-tuned LLMs\n",
            "Researchers  have made pre-trained LLMs easier to use by\n",
            "further training, also known as ﬁne-tuning  (additional\n",
            "training applied on top of the long and costly step above)\n",
            "them on the following:\n",
            "Task-speciﬁc datasets\n",
            "These are datasets of pairs of questions/answers\n",
            "manually assembled by researchers, providingexamples of desirable responses to common questions\n",
            "end-users might prompt the model with. F or example,\n",
            "the dataset might contain the following pair: Q: What\n",
            "is the capital of England? A: The capital of England is\n",
            "London. Unlike the pre-training datasets, these are\n",
            "manually assembled, so they are by necessity much\n",
            "smaller .\n",
            "Reinforcement learning through human feedback (RLHF)\n",
            "Through the use of reinforcement learning  (RL)\n",
            "methods, those manually assembled datasets are\n",
            "augmented with user feedback received on output\n",
            "produced by the model. F or example, user A preferred\n",
            "The capital of England is London  to London is the\n",
            "capital of England  as an answer to the question above.\n",
            "Instruction-tuning has been key to broadening the number\n",
            "of people that can build applications with LLMs, as they\n",
            "can now be prompted with instructions , often in the form of\n",
            "questions like What is the capital of England , as opposed to\n",
            "The capital of England is . This has made it a lot easier to\n",
            "use these models to power chatbot interfaces, for which\n",
            "more specialized models have now been produced, in other\n",
            "words, LLMs tuned speciﬁcally for dialogue, or chat tasks.Dialogue-tuned LLMs\n",
            "Models tailored for dialogue or chat purposes are a further\n",
            "enhancement  of instruction-tuned LLMs. Diﬀerent\n",
            "providers of LLMs use diﬀerent techniques, so this is not\n",
            "necessarily true of all chat models , but usually this is done\n",
            "via the following:\n",
            "Dialogue datasets\n",
            "The manually assembled ﬁne-tuning  datasets are\n",
            "extended to include more examples of multi-turn\n",
            "dialogue interactions, giving models examples of\n",
            "dialogue interactions with back -and-forth between the\n",
            "prompter and the model\n",
            "Chat format\n",
            "The input and output formats of the model are given a\n",
            "layer of structure over freeform text, which divides\n",
            "text into parts associated with a role (and optionally\n",
            "other metadata like a name). Usually the roles\n",
            "available are system  (for instructions and framing of\n",
            "the task), user (the actual task or question), and\n",
            "assistant (for the outputs of the model). This method\n",
            "evolved from early prompt engineering techniques ,and makes it easier to tailor the model’s output while\n",
            "making it harder for models to confuse user input\n",
            "with instructions, also known as jailbreaking , which\n",
            "can, for instance, lead to carefully crafted prompts,\n",
            "possibly a trade secret, being exposed to end-users.\n",
            "Fine-tuned LLMs\n",
            "Here base LLMs are further trained on a proprietary\n",
            "dataset for a speciﬁc task. T echnically , instruction- and\n",
            "dialogue-tuned LLMs are ﬁne-tuned LLMs, but this term is\n",
            "usually taken to mean LLMs that are ﬁne-tuned by the\n",
            "developer for their speciﬁc task. F or example, a model can\n",
            "be ﬁne-tuned to accurately extract the sentiment, risk\n",
            "factors, and key ﬁnancial ﬁgures from a public company’s\n",
            "annual report. Usually ﬁne-tuned models have improved\n",
            "performance on the chosen task, at the expense of a loss of\n",
            "generality . That is, they become less capable of answering\n",
            "queries on unrelated tasks.TIP\n",
            "Here’s a metaphor for ﬁne-tuning. You can train an ordinary dog with\n",
            "basic instructions: stay, come, and sit. These commands are suﬀicient\n",
            "for a dog in normal everyday situations. However, if you need a special-\n",
            "service dog, like a police dog, you’d provide additional special training\n",
            "to help the dog perform specialized tasks.\n",
            "Throughout the rest of this book, when we use the term\n",
            "LLM, we mean instruction-tuned LLMs, and for chat model\n",
            "we mean dialogue-instructed LLMs, as deﬁned earlier in\n",
            "this section. These should be your workhorses when using\n",
            "LLMs—the ﬁrst tools you reach for when starting a new\n",
            "LLM application.\n",
            "Now let’s quickly discuss the main LLM prompting\n",
            "techniques, before diving into LangChain.\n",
            "Brief Primer on Prompting\n",
            "As we touched on earlier , the main task of the software\n",
            "engineer working with LLMs is not to train an LLM, or\n",
            "even to ﬁne-tune one (usually), but rather to take an\n",
            "existing LLM and work out how to get it to accomplish the\n",
            "task you need for your application. There are commercialproviders of LLMs, like OpenAI, Anthropic, and Google, as\n",
            "well as open-source LLMs ( Llama , Gemma , and others),\n",
            "released free-of -charge for others to build upon. Adapting\n",
            "an existing LLM for your task is called prompt engineering .\n",
            "Many prompting techniques have been invented or\n",
            "discovered in the past two years, and in some sense this is\n",
            "a book about how to do prompt engineering, in the\n",
            "broadest sense of the word, with LangChain—how to use\n",
            "LangChain to get LLMs to do what you have in mind. But\n",
            "before we get into LangChain proper , it helps to go over\n",
            "some of these techniques ﬁrst (and we apologize in\n",
            "advance if your favorite prompting technique  isn’t listed\n",
            "here; there are too many to cover).\n",
            "To follow along with this section we recommend copying\n",
            "these prompts to the OpenAI Playground  to try them\n",
            "yourself:\n",
            "1. Create an account for the OpenAI API at\n",
            "http://platform.openai.com . This is distinct from your\n",
            "ChatGPT account, if you have one. This one will let you\n",
            "use OpenAI LLMs programmatically , that is, using the\n",
            "API from your Python or JavaScript code. It will also giveyou access to the OpenAI Playground, where you can\n",
            "experiment with prompts.\n",
            "2. If necessary , add payment details for your new OpenAI\n",
            "account. OpenAI is a commercial provider of LLMs and\n",
            "charges a fee for each time you use their models through\n",
            "the API or Playground. Y ou can ﬁnd the latest pricing\n",
            "here. Over the past two years the price for using\n",
            "OpenAI’s models has come down as new capabilities and\n",
            "optimizations are introduced.\n",
            "3. Head on over to the OpenAI Playground , pick\n",
            "Completions mode, and you’re ready to try out the\n",
            "following prompts for yourself . We’ll make use of the\n",
            "OpenAI API throughout this book.\n",
            "Now on to the prompts!\n",
            "Zero-shot Prompting\n",
            "The ﬁrst and simplest prompting technique consists of just\n",
            "instructing the LLM perform the task at hand:\n",
            "How old was the 30th president of the United StatThis is probably what you should always try ﬁrst, and it will\n",
            "usually work for simple questions, especially when the\n",
            "answer to it was likely present in some of the training data.\n",
            "If we prompt OpenAI’s gpt-3.5-turbo-instruct-0914\n",
            "with the preceding prompt the following:\n",
            "NOTE\n",
            "You may get a diﬀerent result from what we got. There is an element of\n",
            "randomness to how LLMs generate responses, and OpenAI may have\n",
            "updated the model since we tried this ourselves.\n",
            "That answer , the output from the LLM, isn’t quite right.\n",
            "Often you’ll have to iterate on prompts and responses to\n",
            "get a reliable system. The next sections expand on how .\n",
            "Chain-of-thought\n",
            "A very useful iteration is to further instruct the model to\n",
            "take the time to think . Called chain-of -thought (CoT), this is\n",
            "usually done by prepending the prompt with instructions\n",
            "for the LLM to describe how it could arrive at the question:The 30th president of the United States, Calvin CAnd now let’s see what the same model produces for this\n",
            "new prompt:\n",
            "Now we see the model outputs a sequence of steps you\n",
            "could go through when trying to answer this question.\n",
            "Unfortunately , while the steps are very reasonable, the ﬁnal\n",
            "output is still incorrect, given it got some facts wrong. Let’s\n",
            "see how to improve on this.\n",
            "Retrieval-augmented Generation\n",
            "Retrieval-augmented generation (RAG) consists of ﬁnding\n",
            "relevant pieces of text, also known as context , such as facts\n",
            "you’d ﬁnd in an encyclopedia and including that context in\n",
            "the prompt. The RAG technique can (and in realThink step-by step.  \n",
            "How old was the 30th president of the United Stat\n",
            "1. First, find out who the 30th president of the \n",
            "2. Next, find out the year when Calvin Coolidge w\n",
            "3. Now, you need to determine the year when Calvi\n",
            "4. Calculate the age of Calvin Coolidge's wife's \n",
            "5. Determine the age of Calvin Coolidge when his applications should) be combined with CoT , but for\n",
            "simplicity we’ll use these techniques one at a time here.\n",
            "Here’s the prompt including RAG:\n",
            "And the output from the model:\n",
            "Now we’re a lot closer to the correct answer , but as we\n",
            "touched on earlier , LLMs aren’t great at math out-of -the-\n",
            "box. In this case, the result is oﬀ by 3. Let’s keep looking at\n",
            "techniques to see see how we can improve on this.\n",
            "Tool-calling\n",
            "This technique consists of prepending the prompt with a\n",
            "list of external functions the LLM can make use of , alongContext:  \n",
            "- Calvin Coolidge (born John Calvin Coolidge Jr.;\n",
            "- Grace Anna Coolidge (née Goodhue; January 3, 18\n",
            "- Grace Anna Goodhue was born on January 3, 1879,\n",
            "- Lemira A. Goodhue (Barrett) ; Birthdate: April \n",
            "How old was the 30th president of the United Stat\n",
            "The 30th president of the United States, Calvin Cwith descriptions of what each is good for , and instructions\n",
            "on how to signal in the output that it wants  to use one (or\n",
            "more) of these functions. Finally , you, the developer of the\n",
            "application, should parse the output and call the\n",
            "appropriate functions. Here’s one way to do this:\n",
            "And this is the output you might get:\n",
            "While the LLM correctly followed the output format\n",
            "instructions, the tools and inputs selected aren’t the most\n",
            "appropriate for this question. This gets at one of the most\n",
            "important things to keep in mind when prompting LLMs:\n",
            "each prompting technique is most useful when used in\n",
            "combination with (some of) the others . For instance, hereTools: \n",
            "- calculator: This tool accepts math expressions \n",
            "- search: This tool accepts search engine queries\n",
            "If you want to use tools to arrive at the answer,\n",
            "How old was the 30th president of the United Stat\n",
            "tool,input  \n",
            "calculator,2023-1892  \n",
            "search,\"What age was Calvin Coolidge when his motwe could improve on this by combining tool-calling, chain-\n",
            "of-thought, and RAG into a prompt that uses all three. Let’s\n",
            "see what that looks like:\n",
            "And with this prompt, maybe after a few tries, we might get\n",
            "this output:\n",
            "tool,input  \n",
            "calculator,1929 - 1872\n",
            "If we parse that CSV output, and have a calculator function\n",
            "execute the operation 1929 - 1827, we ﬁnally get the right\n",
            "answer: 57 years.Context:  \n",
            "- Calvin Coolidge (born John Calvin Coolidge Jr.;\n",
            "- Grace Anna Coolidge (née Goodhue; January 3, 18\n",
            "- Grace Anna Goodhue was born on January 3, 1879,\n",
            "- Lemira A. Goodhue (Barrett) ; Birthdate: April \n",
            "Tools: \n",
            "- calculator: This tool accepts math expressions \n",
            "If you want to use tools to arrive at the answer,\n",
            "Think step-by step.  \n",
            "How old was the 30th president of the United StatFew-shot Prompting\n",
            "Finally , we come to another very useful prompting\n",
            "technique: few-shot prompting . This consists of providing\n",
            "the LLM with examples of other questions and the correct\n",
            "answers, which enables the LLM to learn  how to perform a\n",
            "new task without going through additional training or ﬁne-\n",
            "tuning. When compared to ﬁne-tuning, few-shot prompting\n",
            "is more ﬂexible—you can do it on-the-ﬂy at query time—but\n",
            "less powerful, and you might achieve better performance\n",
            "with ﬁne-tuning. That said, you should usually always try\n",
            "few-shot prompting before ﬁne-tuning.\n",
            "Static few-shot prompting\n",
            "The most basic version of few-shot prompting is to\n",
            "assemble a predetermined list of a small number of\n",
            "examples which you include in the prompt.\n",
            "Dynamic few-shot prompting\n",
            "If you assemble a dataset of many examples, you can\n",
            "instead pick the best examples for each new query\n",
            "(best here usually means most relevant ).\n",
            "The next section covers using LangChain to build\n",
            "applications using LLMs and these prompting techniques.LangChain and Why It’s\n",
            "Important\n",
            "LangChain was the ﬁrst (and is still today the largest)\n",
            "library to provide LLM and prompting building blocks and\n",
            "the tooling to reliably combine them into larger\n",
            "applications. T oday, LangChain has amassed over 7 million\n",
            "monthly downloads, 82,000  GitHub stars, and the largest\n",
            "developer community in generative AI ( 72,000+ strong ). It\n",
            "has enabled software engineers who don’t have an ML\n",
            "background to utilize the power of LLMs to build a variety\n",
            "of apps, ranging from AI chatbots to AI agents that can\n",
            "reason and take action responsibly .\n",
            "LangChain builds on the idea stressed in the preceding\n",
            "section: that prompting techniques are most useful when\n",
            "used together . To make that easier , LangChain provides\n",
            "simple abstractions , for each major prompting technique.\n",
            "By abstraction we mean Python and JavaScript functions\n",
            "and classes that encapsulate the ideas of those techniques\n",
            "into easy-to-use wrappers. These abstractions are designed\n",
            "to play well together and to be combined into a larger LLM\n",
            "application.First of all, LangChain provides integrations with the major\n",
            "LLM providers, both commercial ( OpenAI , Anthropic ,\n",
            "Google , and more) and open-source ( Llama , Gemma , and\n",
            "others). These integrations share a common interface,\n",
            "making it very easy to try out new LLMs as they’re\n",
            "announced and letting you avoid being locked-in to a single\n",
            "provider . We’ll use these in Chapter 1.\n",
            "LangChain also provides prompt template  abstractions,\n",
            "which enable you to reuse prompts more than once,\n",
            "separating what’s static text in the prompt from\n",
            "placeholders that will be diﬀerent for each time you send it\n",
            "to the LLM to get a completion generated. W e’ll talk more\n",
            "about these also in Chapter 1. LangChain prompts can also\n",
            "be stored in the LangChain Hub for sharing with\n",
            "teammates.\n",
            "LangChain contains many integrations with third-party\n",
            "services (like Google Sheets, W olfram Alpha, Zapier , just to\n",
            "name a few) exposed as tools , which is a standard interface\n",
            "for functions to be used in the tool-calling technique.\n",
            "For RAG , LangChain provides integrations with the major\n",
            "embedding models  (language models designed to output a\n",
            "numeric representation, the embedding , of the meaning ofa sentence, paragraph, and so on), vector stores  (databases\n",
            "dedicated to storing embeddings), and vector indexes\n",
            "(regular databases with vector storing capabilities). Y ou’ll\n",
            "learn a lot more about these in Chapters 2 and 3.\n",
            "For CoT , LangChain provides agent  abstractions which\n",
            "combine chain-of -thought reasoning and tool-calling, ﬁrst\n",
            "popularized by the ReAct  paper . This enables building LLM\n",
            "applications that do the following:\n",
            "1. Reason about the steps to take.\n",
            "2. Translate those steps into external tool calls.\n",
            "3. Receive the output of those tool calls.\n",
            "4. Repeat until the task is accomplished.\n",
            "We cover these in Chapters 5 and 6.\n",
            "For chatbot use cases, it becomes useful to keep track of\n",
            "previous interactions and use them when generating the\n",
            "response to a future interaction. This is called memory , and\n",
            "Chapter 4 discusses using it in LangChain.\n",
            "Finally , LangChain provides the tools to compose these\n",
            "building blocks into cohesive applications. Chapters 1\n",
            "through 6 talk more about this.In addition to this library , LangChain provides LangSmith , a\n",
            "platform to help debug, test, deploy , and monitor AI\n",
            "workﬂows, and LangServe , a platform that makes it easier\n",
            "to deploy a LangChain-powered API. W e cover these in\n",
            "chapters 7 and 8.\n",
            "What to Expect from This Book\n",
            "With this book we hope to convey the excitement and\n",
            "possibility of adding LLMs to your software engineering\n",
            "toolbelt.\n",
            "I got into programming because I like building things,\n",
            "getting to the end of a project, looking at the ﬁnal product\n",
            "and realizing there’s something new out there, and I built\n",
            "it. Programming with LLMs is so exciting to me because it\n",
            "expands the set of things I can build, it makes previously\n",
            "hard things easy (for example, extracting relevant numbers\n",
            "from a long text) and previously impossible things possible\n",
            "— try building an automated assistant a year ago and you\n",
            "end up with the phone tree hell  we all know and love from\n",
            "calling up customer support numbers.Now with LLMs and LangChain you can actually build\n",
            "pleasant assistants (or myriad other applications) that chat\n",
            "with you and understand your intent to a very reasonable\n",
            "degree. The diﬀerence is night and day! If that sounds\n",
            "exciting to you (as it does to us) then you’ve come to the\n",
            "right place.\n",
            "In this preface we’ve given you a refresher on what makes\n",
            "LLMs tick, and why exactly that gives you “thing building”\n",
            "superpowers. Having these very large ML models that\n",
            "understand language and can output answers written in\n",
            "conversational English (or some other language) gives you\n",
            "a programmable  (through prompt engineering), versatile\n",
            "language-generation tool. By the end of the book we hope\n",
            "you’ll see just how powerful that can be.\n",
            "We’ll begin with an AI chatbot customized by , for the most\n",
            "part, plain English instructions. That alone should be an\n",
            "eye-opener , that you can now “program” part of the\n",
            "behavior of your application without code.\n",
            "Then comes the next capability: giving your chatbot access\n",
            "to your own documents, which takes it from a generic\n",
            "assistant to one that’s knowledgeable about any area of\n",
            "human knowledge for which you can ﬁnd a library ofwritten text. This will allow you to have the chatbot answer\n",
            "questions or summarize documents you wrote, for instance.\n",
            "After that, we’ll make the chatbot remember your previous\n",
            "conversations. This will improve it in two ways: It will feel a\n",
            "lot more natural to have a conversation with a chatbot that\n",
            "remembers what you have previously chatted about, and\n",
            "over time the chatbot can be personalized to the\n",
            "preferences of each of its users individually .\n",
            "Next, we’ll use chain-of -thought and tool-calling techniques\n",
            "to give the chatbot the ability to plan and act on those\n",
            "plans, iteratively . This will enable it to work towards more\n",
            "complicated requests, such as writing a research report\n",
            "about a subject of your choice.\n",
            "As you use your chatbot for more complicated tasks you’ll\n",
            "feel the need to give it the tools to collaborate with you\n",
            "towards it. This encompasses both giving you the ability to\n",
            "interrupt or authorize actions before they are taken, as well\n",
            "as providing the chatbot with the ability to ask for more\n",
            "information or clariﬁcation before acting.\n",
            "Finally , we’ll show you how to deploy your chatbot to\n",
            "production, and discuss what you need to consider beforeand after taking that step, including latency , reliability , and\n",
            "security . And then we’ll show you how to monitor your\n",
            "chatbot in production and continue to improve it as it is\n",
            "used.\n",
            "Along the way we’ll teach you the ins and outs of each of\n",
            "these techniques, so that when you ﬁnish the book you will\n",
            "have truly added a new tool (or two) to your software\n",
            "engineering toolbelt.Chapter 1. LLM Fundamentals\n",
            "with LangChain\n",
            "A NOTE FOR EARLY RELEASE READERS\n",
            "With Early Release ebooks, you get books in their earliest\n",
            "form—the author’s raw and unedited content as they write\n",
            "—so you can take advantage of these technologies long\n",
            "before the oﬀicial release of these titles.\n",
            "This will be the 1st chapter of the ﬁnal book. Please note\n",
            "that the GitHub repo will be made active later on.\n",
            "If you have comments about how we might improve the\n",
            "content and/or examples in this book, or if you notice\n",
            "missing material within this chapter , please reach out to\n",
            "the editor at ccollins@oreilly .com .\n",
            "The preface gave you a taste of the power of LLM\n",
            "prompting, where we saw ﬁrst-hand the impact each\n",
            "prompting technique can have on what you get out of\n",
            "LLMs, especially when judiciously combined. The challenge\n",
            "in building good LLM applications is, in fact, in how to\n",
            "eﬀectively construct the prompt sent to the model andprocess the model’s prediction to return an accurate output\n",
            "(Figure 1-1 )\n",
            "Figure 1-1. The challenge in making LLMs a useful part of your application.\n",
            "If you can solve this problem, you are well on your way to\n",
            "build LLM applications, simple and complex alike. In this\n",
            "chapter , you’ll learn more about how LangChain’s building\n",
            "blocks map to LLM concepts and how , when combined\n",
            "eﬀectively , they enable you to build LLM applications. But\n",
            "ﬁrst, a brief primer on why we think it useful to use\n",
            "LangChain to build LLM applications.WHY LANGCHAIN?\n",
            "You can of course build LLM applications without\n",
            "LangChain. The most obvious alternative is to use the SDK\n",
            "—the software package exposing the methods of their\n",
            "HTTP API as functions in the programming language of\n",
            "your choice—of the LLM provider you tried ﬁrst, for\n",
            "example, OpenAI. W e think learning LangChain will pay oﬀ\n",
            "in the short term and over the long run, and wanted to oﬀer\n",
            "a few words on why that is.\n",
            "Prebuilt common patterns\n",
            "LangChain comes with reference implementations of\n",
            "the most common LLM application patterns (we\n",
            "mentioned some of these in the preface: chain-of -\n",
            "thought, tool calling, and so on). This is the quickest\n",
            "way to get started with LLMs, and might often be all\n",
            "you need. W e’d suggest starting any new application\n",
            "from these and checking whether the results out of\n",
            "the box are good enough for your use case. If not,\n",
            "then look at the other half of the LangChain libraries:\n",
            "Interchangeable building blocks\n",
            "Components that can be easily swapped out for\n",
            "alternatives. Every component (an LLM, chat model,output parser , and so on—more on these shortly)\n",
            "follows a shared speciﬁcation, which makes your\n",
            "application future-proof . As new capabilities are\n",
            "released by model providers, and as your needs\n",
            "change, you can evolve your application without\n",
            "rewriting it each time.\n",
            "Throughout this book we make use of the following major\n",
            "components in the code examples:\n",
            "LLM/chat model: OpenAI\n",
            "Embeddings: OpenAI\n",
            "Vector store: Pgvector\n",
            "You can swap out each of these for any of the alternatives\n",
            "listed on the following pages:\n",
            "Chat models\n",
            "See here. I f you don’t want to use OpenAI (a\n",
            "commercial API) we suggest Anthropic  as a\n",
            "commercial alternative or Ollama  as an open source\n",
            "one.\n",
            "EmbeddingsSee here. If you don’t want to use OpenAI (a\n",
            "commercial API) we suggest Cohere  as a commercial\n",
            "alternative or Ollama  as an open source one.\n",
            "Vector stores\n",
            "See here. If you don’t want to use Pgvector (an open\n",
            "source extension to the popular SQL database\n",
            "Postgres) we suggest using either Weaviate  (a\n",
            "dedicated vector store) or OpenSearch  (vector search\n",
            "features that are part of a popular search database).\n",
            "This eﬀort goes beyond just, for instance, all LLMs having\n",
            "the same methods, with similar arguments and return\n",
            "values. Let’s look at the example of chat models, and two\n",
            "popular LLM providers, OpenAI and Anthropic. Both have a\n",
            "chat API which receives chat messages  (loosely deﬁned as\n",
            "objects with a type string and a content string) and returns\n",
            "a new message generated by the model. But if you try to\n",
            "use both models in the same conversation you’ll\n",
            "immediately run into issues, as their chat message formats\n",
            "are subtly incompatible. LangChain abstracts away these\n",
            "diﬀerences to enable building applications that are truly\n",
            "independent of a particular provider . For instance, with\n",
            "LangChain a chatbot conversation where you use both\n",
            "OpenAI and Anthropic models just works.Finally , as you build out your LLM applications with several\n",
            "of these components we’ve found it useful to have the\n",
            "orchestration  capabilities of LangChain:\n",
            "All major components are instrumented by the callbacks\n",
            "system for observability (more on this in Chapter 8).\n",
            "All major components implement the same interface\n",
            "(more on this towards the end of this chapter).\n",
            "Long-running LLM applications can be interrupted,\n",
            "resumed, or retried (more on this in Chapter 6).\n",
            "Getting Set Up with LangChain\n",
            "To follow along with the rest of the chapter , and the\n",
            "chapters to come, we recommend setting up LangChain on\n",
            "your computer ﬁrst.\n",
            "First, see the instructions in the preface regarding setting\n",
            "up an OpenAI account and complete these ﬁrst if you\n",
            "haven’t yet. If you prefer using a diﬀerent LLM provider ,\n",
            "see the nearby sidebar for alternatives.\n",
            "Then head on over the API K eys page on the OpenAI\n",
            "website (after logging in to your account), create an APIkey, and save it—you’ll need it soon.\n",
            "NOTE\n",
            "In this book we’ll show code examples in both Python and JavaScript,\n",
            "LangChain oﬀers the same functionality in both languages, so just pick\n",
            "one of them and follow the respective code snippets throughout the\n",
            "book. The code examples for each language will be equivalent between\n",
            "the two, so just pick whichever language you’re most comfortable with.\n",
            "First, some setup instructions for readers using Python:\n",
            "1. Ensure you have Python installed. See instructions here\n",
            "for your operating system.\n",
            "2. Install Jupyter if you want to run the examples in a\n",
            "notebook environment. Y ou can do this by running pip\n",
            "install notebook  in your terminal.\n",
            "3. Install the LangChain library by running the following\n",
            "command in your terminal: pip install langchain\n",
            "langchain_openai langchain_community langchain-\n",
            "text-splitters langchain-postgres\n",
            "4. Take the OpenAI API key you generated at the beginning\n",
            "of this section and make it available in your terminal\n",
            "environment. Y ou can do this by running the following:\n",
            "export OPENAI_API_KEY=your-key . Don’t forget toreplace your-key  with the API key you generated\n",
            "previously .\n",
            "5. Open a Jupyter notebook by running this command:\n",
            "jupyter notebook . You’re now ready to follow along\n",
            "with the Python code examples.\n",
            "And now some instructions for readers using JavaScript:\n",
            "1. Take the OpenAI API key you generated at the beginning\n",
            "of this section and make it available in your terminal\n",
            "environment. Y ou can do this by running the following:\n",
            "export OPENAI_API_KEY=your-key . Don’t forget to\n",
            "replace your-key  with the API key you generated\n",
            "previously .\n",
            "2. If you want to run the examples as Node.js scripts, install\n",
            "Node following instructions here.\n",
            "3. Install the LangChgain libraries by running the following\n",
            "command in your terminal: npm install langchain\n",
            "@langchain/openai @langchain/community pg\n",
            "4. Take each example, save it as a .js ﬁle and run it with\n",
            "node ./file.js .\n",
            "Using LLMs in LangChainFirst, to recap, as you know by now , LLMs are the driving\n",
            "engine behind most generative AI applications. LangChain\n",
            "provides two simple interfaces to interact with any LLM\n",
            "API provider:\n",
            "LLMs\n",
            "Chat models\n",
            "Let’s start with the ﬁrst one. The LLMs interface simply\n",
            "takes a string prompt as input, sends the input to the model\n",
            "provider , and then returns the model prediction as output.\n",
            "Let’s import LangChain’s OpenAI LLM wrapper to invoke\n",
            "a model prediction using a simple prompt. First in Python:\n",
            "And now in JS:from langchain_openai.llms import OpenAI  \n",
            "model = OpenAI(model='  gpt-3.5-turbo-instruct')  \n",
            "prompt = 'The sky is'  \n",
            "completion = model.invoke(prompt)  \n",
            "completion\n",
            "import {OpenAI} from '@langchain/openai'  \n",
            "const model = new OpenAI({model: 'gpt-3.5-turbo-i\n",
            "const prompt = 'The sky is'  And the output:\n",
            "Blue!const completion = await model.invoke(prompt)  \n",
            "completionTIP\n",
            "Notice the parameter model passed to OpenAI. This is the most\n",
            "common parameter to conﬁgure when using an LLM or Chat Model, the\n",
            "underlying model to use, as most providers oﬀer several models, with\n",
            "diﬀerent trade-oﬀs in capability and cost (usually larger models are\n",
            "more capable, but also more expensive and slower). See here for an\n",
            "overview of the models oﬀered by OpenAI.\n",
            "Other useful parameters to conﬁgure include the following, oﬀered by\n",
            "most providers:\n",
            "temperature\n",
            "This one controls the sampling algorithm used to generate\n",
            "output. Lower values produce more predictable outputs (for\n",
            "example, 0.1), while higher values generate more creative, or\n",
            "unexpected, results (such as 0.9). Diﬀerent tasks will need\n",
            "diﬀerent values for this parameter. For instance, producing\n",
            "structured output usually beneﬁts from a lower temperature,\n",
            "whereas creative writing tasks do better with a higher value.\n",
            "max_tokens\n",
            "This one limits the size (and cost) of the output. A lower value\n",
            "may cause the LLM to stop generating the output before getting\n",
            "to a natural end, so it may appear to have been truncated.\n",
            "Beyond these, each provider exposes a diﬀerent set of parameters. We\n",
            "recommend looking at the documentation for the one you choose. For\n",
            "example, you can see OpenAI’s here.Alternatively , the Chat Model interface enables back and\n",
            "forth conversations between the user and model. The\n",
            "reason why it’s a separate interface is because popular\n",
            "LLM providers like OpenAI diﬀerentiate messages sent to\n",
            "and from the model into user , assistant, and system roles\n",
            "(here role denotes the type of content the message\n",
            "contains):\n",
            "System role\n",
            "Enables the developer to specify instructions the\n",
            "model should use to answer a user question.\n",
            "User role\n",
            "The individual asking questions and generating the\n",
            "queries sent to the model.\n",
            "Assistant role\n",
            "The model’s responses to the user’s query .\n",
            "The chat models interface makes it easier to conﬁgure and\n",
            "manage conversions in your AI chatbot application. Here’s\n",
            "an example utilizing LangChain’s ChatOpenAI model, ﬁrst\n",
            "in Python:\n",
            "from langchain_openai.chat_models import ChatOpenAnd now in JS:\n",
            "And the output:\n",
            "Instead of a single prompt string, chat models make use of\n",
            "diﬀerent types of chat message interfaces associated with\n",
            "each role mentioned previously . These include the\n",
            "following:\n",
            "HumanMessagefrom langchain_core.messages import HumanMessage  \n",
            "model = ChatOpenAI()  \n",
            "prompt = [HumanMessage('What is the capital of Fr\n",
            "completion = model.invoke(prompt)\n",
            "import {ChatOpenAI} from '@langchain/openai'  \n",
            "import {HumanMessage} from '@langchain/core/messa\n",
            "const model = new ChatOpenAI()  \n",
            "const prompt = [new HumanMessage('What is the cap\n",
            "const completion = await model.invoke(prompt)  \n",
            "completion\n",
            "AIMessage(content='The capital of France is ParisA message sent from the perspective of the human,\n",
            "with the user role.\n",
            "AIMessage\n",
            "A message sent from the perspective of the AI the\n",
            "human is interacting with, with the assistant  role.\n",
            "SystemMessage\n",
            "A message setting the instructions the AI should\n",
            "follow , with the system  role.\n",
            "ChatMessage\n",
            "A message allowing for arbitrary setting of role.\n",
            "Let’s incorporate a SystemMessage  instruction in our\n",
            "example, ﬁrst in Python:\n",
            "from langchain_core.messages import AIMessage, Hu\n",
            "from langchain_openai.chat_models import ChatOpen\n",
            "model = ChatOpenAI()  \n",
            "system_msg = SystemMessage('You are a helpful ass\n",
            "human_msg = HumanMessage('What is the capital of \n",
            "completion = model.invoke([system_msg, human_msg]\n",
            "completionAnd now in JS:\n",
            "And the output:\n",
            "AIMessage('Paris!!!')\n",
            "As you can see, the model obeyed the instruction provided\n",
            "in the SystemMessage  even though it wasn’t present in the\n",
            "user’s question. This enables you to pre-conﬁgure your AI\n",
            "application to respond in a relatively predictable manner\n",
            "based on the user’s input.\n",
            "Making LLM prompts reusableimport {ChatOpenAI} from '@langchain/openai'  \n",
            "import {HumanMessage, SystemMessage} from '@langc\n",
            "const model = new ChatOpenAI()  \n",
            "const prompt = [  \n",
            "  new SystemMessage('You are a helpful assistant \n",
            "  new HumanMessage('What is the capital of France\n",
            "] \n",
            "const completion = await model.invoke(prompt)  \n",
            "completionThe previous section showed how the prompt  instruction\n",
            "signiﬁcantly inﬂuences the model’s output. Prompts help\n",
            "the model understand context and generate relevant\n",
            "answers to queries.\n",
            "Here is an example of a detailed prompt:\n",
            "Although the prompt looks like a simple string, the\n",
            "challenge is ﬁguring out what the text should contain, and\n",
            "how it should vary based on the user’s input. In this\n",
            "example, the Context and Question values are hardcoded,\n",
            "but what if we wanted to pass these in dynamically?\n",
            "Fortunately , LangChain provides prompt template\n",
            "interfaces that make it easy to construct prompts with\n",
            "dynamic inputs, ﬁrst in Python:Answer the question based on the context below. I\n",
            "Context: The most recent advancements in NLP are \n",
            "Question: Which model providers offer LLMs?  \n",
            "Answer:\n",
            "from langchain_core.prompts import PromptTemplate\n",
            "template = PromptTemplate.from_template(\"\"\"Answer\n",
            "Context: {context}  And in JS:\n",
            "And the output:Question: {question}  \n",
            "Answer: \"\"\")  \n",
            "prompt = template.invoke({  \n",
            "    \"context\": \"The most recent advancements in N\n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})\n",
            "import {PromptTemplate} from '@langchain/core/pro\n",
            "const template = PromptTemplate.fromTemplate(`Ans\n",
            "Context: {context}  \n",
            "Question: {question}  \n",
            "Answer: `)  \n",
            "const prompt = await template.invoke({  \n",
            "  context: \"The most recent advancements in NLP a\n",
            "  question: \"Which model providers offer LLMs?\"  \n",
            "})\n",
            "StringPromptValue(text='Answer the question basedThat example takes the static prompt from the previous\n",
            "block and makes it dynamic. The template  contains the\n",
            "structure of the ﬁnal prompt alongside the deﬁnition of\n",
            "where the dynamic inputs will be inserted.\n",
            "As such, the template can be used as a recipe to build\n",
            "multiple static, speciﬁc prompts. When you format the\n",
            "prompt with some speciﬁc values, here context  and\n",
            "question , you get a static prompt ready to be passed in to\n",
            "an LLM.\n",
            "As you can see, the question argument is passed\n",
            "dynamically via the invoke  function. By default,\n",
            "LangChain prompts follow Python’s f -string syntax for\n",
            "deﬁning dynamic parameters – any word surrounded by\n",
            "curly braces, such as {question}, are placeholders for\n",
            "values passed in at run-time. In the example above,\n",
            "{question}  was replaced by “Which model providers\n",
            "offer LLMs?” .\n",
            "Let’s see how we’d feed this into an LLM OpenAI model\n",
            "using LangChain, ﬁrst in Python:\n",
            "from langchain_openai.llms import OpenAI  \n",
            "from langchain_core.prompts import PromptTemplate\n",
            "#both`template` and`model` canbereusedmanyAnd in JS:# both template  and model can be reused many \n",
            "template = PromptTemplate.from_template(\"\"\"Answer\n",
            "Context: {context}  \n",
            "Question: {question}  \n",
            "Answer: \"\"\")  \n",
            "model = OpenAI()  \n",
            "# `prompt` and `completion` are the results of us\n",
            "prompt = template.invoke({  \n",
            "    \"context\": \"The most recent advancements in N\n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "}) \n",
            "completion = model.invoke(prompt)\n",
            "import {PromptTemplate} from '@langchain/core/pro\n",
            "import {OpenAI} from '@langchain/openai'  \n",
            "const model = new OpenAI()  \n",
            "const template = PromptTemplate.fromTemplate(`Ans\n",
            "Context: {context}  \n",
            "Question: {question}  \n",
            "Answer: `)  \n",
            "const prompt = await template.invoke({  \n",
            "  context: \"The most recent advancements in NLP a\n",
            "  question: \"Which model providers offer LLMs?\"  \n",
            "}) \n",
            "const completion = await model.invoke(prompt)And the output:\n",
            "If you’re looking to build an AI chat application, the chat\n",
            "prompt template can be used instead to provide dynamic\n",
            "inputs based on the role of the chat message. First in\n",
            "Python:\n",
            "And in JS:Hugging Face's `transformers` library, OpenAI usi\n",
            "from langchain_core.prompts import ChatPromptTemp\n",
            "template = ChatPromptTemplate.from_messages([  \n",
            "    ('system', 'Answer the question based on the \n",
            "    ('human', 'Context: {context}'),  \n",
            "    ('human', 'Question: {question}'),  \n",
            "]) \n",
            "prompt = template.invoke({  \n",
            "    \"context\": \"The most recent advancements in N\n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})\n",
            "import{ChatPromptTemplate} from'@langchain/coreAnd the output:\n",
            "Notice how the prompt contains instructions in a\n",
            "SystemMessage and two HumanMessages that contain\n",
            "dynamic context  and question  variables. Y ou can still\n",
            "format the template in the same way , and get back a static\n",
            "prompt that you can pass to a large language model for a\n",
            "prediction output. First in Python:pot {Catoptepate} o @agca/coe\n",
            "const template = ChatPromptTemplate.fromMessages\n",
            "    ['system', 'Answer the question based on the \n",
            "    ['human', 'Context: {context}'],  \n",
            "    ['human', 'Question: {question}'],  \n",
            "]) \n",
            "const prompt = await template.invoke({  \n",
            "  context: \"The most recent advancements in NLP a\n",
            "  question: \"Which model providers offer LLMs?\"  \n",
            "})\n",
            "ChatPromptValue(messages=[SystemMessage(content=\n",
            "from langchain_openai.chat_models import ChatOpen\n",
            "from langchain_core.prompts import ChatPromptTemp\n",
            "# both `template` and `model` can be reused many \n",
            "template = ChatPromptTemplate.from_messages([  And in JS:    ('system', 'Answer the question based on the \n",
            "    ('human', 'Context: {context}'),  \n",
            "    ('human', 'Question: {question}'),  \n",
            "]) \n",
            "model = ChatOpenAI()  \n",
            "# `prompt` and `completion` are the results of us\n",
            "prompt = template.invoke({  \n",
            "    \"context\": \"The most recent advancements in N\n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "}) \n",
            "completion = model.invoke(prompt)\n",
            "import {ChatPromptTemplate} from '@langchain/core\n",
            "import {ChatOpenAI} from '@langchain/openai'  \n",
            "const model = new ChatOpenAI()  \n",
            "const template = ChatPromptTemplate.fromMessages\n",
            "    ['system', 'Answer the question based on the \n",
            "    ['human', 'Context: {context}'],  \n",
            "    ['human', 'Question: {question}'],  \n",
            "]) \n",
            "const prompt = await template.invoke({  \n",
            "  context: \"The most recent advancements in NLP a\n",
            "  question: \"Which model providers offer LLMs?\"  \n",
            "}) \n",
            "const completion = await model.invoke(prompt)And the output:\n",
            "Getting Speciﬁc Formats out of\n",
            "LLMs\n",
            "Plain text outputs are useful, but there may be use cases\n",
            "where you need the LLM to generate a structured  output,\n",
            "that is, output in a machine-readable format, such as JSON ,\n",
            "XML or CSV , or even in a programming language such as\n",
            "Python or JavaScript. This is very useful when you intend to\n",
            "hand that output oﬀ to some other piece of code, making an\n",
            "LLM play a part in your larger application.\n",
            "JSON Output\n",
            "The most common format to generate with LLMs is JSON ,\n",
            "which can then be used to, for instance:\n",
            "Send it over the wire to your frontend code\n",
            "Saving it to a databaseAIMessage(content=\"Hugging Face's `transformers` When generating JSON , the ﬁrst task is to deﬁne the\n",
            "schema you want the LLM to respect when producing the\n",
            "output. Then, you should include that schema in the\n",
            "prompt, along with the text you want to use as the source.\n",
            "Let’s see an example, ﬁrst in Python:\n",
            "And in JS:from langchain_openai import ChatOpenAI  \n",
            "from langchain_core.pydantic_v1 import BaseModel  \n",
            "class AnswerWithJustification(BaseModel):  \n",
            "    '''An answer to the user question along with \n",
            "    answer: str  \n",
            "    '''The answer to the user's question'''  \n",
            "    justification: str  \n",
            "    '''Justification for the answer'''  \n",
            "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\", temp\n",
            "structured_llm = llm.with_structured_output(Answe\n",
            "structured_llm.invoke(\"What weighs more, a pound \n",
            "import {ChatOpenAI} from '@langchain/openai'  \n",
            "import {z} from \"zod\";  \n",
            "const answerSchema = z  \n",
            "  .object({  \n",
            "    answer: z.string().describe(\"The answer to thAnd the output:\n",
            "So, ﬁrst deﬁne a schema. In Python this is easiest to do\n",
            "with Pydantic (a library used for validating data against\n",
            "schemas). In JS this is easiest to do with Zod (an equivalent\n",
            "library). The method with_structured_output  will use\n",
            "that schema for two things:\n",
            "The schema will be converted to a JSONSchema  object (a\n",
            "JSON format used to describe the shape, that is types,\n",
            "names, descriptions, of JSON data), which will be sent to\n",
            "the LLM. F or each LLM, LangChain picks the best method\n",
            "to do this, usually function-calling or prompting.    justification: z.string().describe(\"Justifica\n",
            "  }) \n",
            "  .describe(\"An answer to the user question along\n",
            "const model = new ChatOpenAI({model: \"gpt-3.5-tur\n",
            "await model.invoke(\"What weighs more, a pound of \n",
            "{ \n",
            "  answer: \"They weigh the same\",  \n",
            "  justification: \"Both a pound of bricks and a po\n",
            "}The schema will also be used to validate the output\n",
            "returned by the LLM before returning it, this ensures the\n",
            "output produced respects the schema you passed in exactly .\n",
            "Other Machine-Readable Formats with\n",
            "Output Parsers\n",
            "You can also use an LLM or Chat Model to produce output\n",
            "in other formats, such as CSV or XML. This is where output\n",
            "parsers come in handy . Output parsers  are classes that help\n",
            "you structure large language model responses. They serve\n",
            "two functions:\n",
            "Providing format instructions\n",
            "Output parsers can be used to inject some additional\n",
            "instructions in the prompt that help guide the LLM to\n",
            "output text in the format it knows how to parse.\n",
            "Validating and parsing output\n",
            "The main function is to take the textual output of the\n",
            "LLM or Chat Model and render it to a more structured\n",
            "format, such as a list, XML, and so on. This can\n",
            "include removing extraneous information, correcting\n",
            "incomplete output, and validating the parsed values.Here’s an example of how an output parser works, ﬁrst in\n",
            "Python:\n",
            "And in JS:\n",
            "And the output:\n",
            "['apple', 'banana', 'cherry']\n",
            "LangChain provides a variety of output parsers for various\n",
            "use cases, including CSV , XML, and more. W e’ll see how to\n",
            "combine output parsers with models and prompts in the\n",
            "next section.from langchain_core.output_parsers import CommaSe\n",
            "parser = CommaSeparatedListOutputParser()  \n",
            "items = parser.invoke(\"apple, banana, cherry\")\n",
            "import {CommaSeparatedListOutputParser} from '@la\n",
            "const parser = new CommaSeparatedListOutputParser\n",
            "await parser.invoke(\"apple, banana, cherry\")Assembling the Many Pieces of\n",
            "an LLM Application\n",
            "The key components you’ve learned about so far are\n",
            "essential building blocks of the LangChain framework.\n",
            "Which brings us to the critical question: how do you\n",
            "combine them eﬀectively to build your LLM application?\n",
            "Using the Runnable Interface\n",
            "As you may have noticed, all the code examples used so far\n",
            "utilize a similar interface and the invoke()  method to\n",
            "generate outputs from the model (or prompt template, or\n",
            "output parser). All components have the following:\n",
            "A common interface with these methods:\n",
            "invoke\n",
            "Transforms a single input into an output.\n",
            "batch\n",
            "Eﬀiciently transforms multiple inputs into multiple\n",
            "outputs.\n",
            "streamStreams output from a single input as it’s produced.\n",
            "Built-in utilities for retries, fallbacks, schemas, and\n",
            "runtime conﬁgurability\n",
            "In Python, each of the 3 methods above have asyncio\n",
            "equivalents.\n",
            "As such all components behave the same way , and the\n",
            "interface learned for one of them applies to all. First in\n",
            "Python:\n",
            "And in JS:from langchain_openai.llms import OpenAI  \n",
            "model = OpenAI()  \n",
            "completion = model.invoke('Hi there!')  \n",
            "# Hi! \n",
            "completions = model.batch(['Hi there!', 'Bye!'])  \n",
            "# ['Hi!', 'See you!']  \n",
            "for token in model.stream('Bye!'):  \n",
            "    print(token)  \n",
            "    # Good  \n",
            "    # bye  \n",
            "    # !There you see how the three main methods work:\n",
            "invoke()  takes a single input and returns a single\n",
            "output.\n",
            "batch()  takes a list of outputs and returns a list of\n",
            "outputs.\n",
            "stream()  takes a single input and returns an iterator of\n",
            "parts of the output as they become available.\n",
            "In some cases, if the underlying component doesn’t support\n",
            "iterative output, there will be a single part containing all\n",
            "output.import {OpenAI} from '@langchain/openai'  \n",
            "const model = new OpenAI()  \n",
            "const completion = await model.invoke('Hi there!\n",
            "// Hi! \n",
            "const completions = await model.batch(['Hi there\n",
            "// ['Hi!', 'See you!']  \n",
            "for await (const token of await model.stream('Bye\n",
            "  console.log(token)  \n",
            "  // Good  \n",
            "  // bye  \n",
            "  // ! \n",
            "}You can combine these components in two ways:\n",
            "Imperative\n",
            "Call them directly , for example with\n",
            "model.invoke(...)\n",
            "Declarative\n",
            "With LangChain Expression Language (LCEL),\n",
            "covered in an upcoming section.\n",
            "Table 2-1 summarizes their diﬀerences, and we’ll see each\n",
            "in action next.Table 1-1. The main diﬀerences between Imperative and Declarative\n",
            "composition.\n",
            "Imperative Declarative\n",
            "Syntax All of Python or JavaScript LCEL\n",
            "Parallel\n",
            "executionPython: W ith threads or\n",
            "coroutines  \n",
            "JavaScript: W ith Promise.\n",
            "allAutomatic\n",
            "Streaming With yield keyword Automatic\n",
            "Async\n",
            "executionWith async functions Automatic\n",
            "Imperative Composition\n",
            "Imperative composition  is just a fancy name for writing the\n",
            "code you’re used to writing, composing these components\n",
            "into functions and classes. Here’s an example combining\n",
            "prompts, models, and output parsers, ﬁrst in Python:\n",
            "from langchain_openai.chat_models import ChatOpen\n",
            "from langchain_core.prompts import ChatPromptTemp\n",
            "from langchain_core.runnables import chain  \n",
            "# the building blocks  And in JS:template = ChatPromptTemplate.from_messages([  \n",
            "    ('system', 'You are a helpful assistant.'),  \n",
            "    ('human', '{question}'),  \n",
            "]) \n",
            "model = ChatOpenAI()  \n",
            "# combine them in a function  \n",
            "# @chain decorator adds the same Runnable interfa\n",
            "@chain \n",
            "def chatbot(values):  \n",
            "    prompt = template.invoke(values)  \n",
            "    return model.invoke(prompt)  \n",
            "# use it  \n",
            "result = chatbot.invoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})\n",
            "import {ChatOpenAI} from '@langchain/openai'  \n",
            "import {ChatPromptTemplate} from '@langchain/core\n",
            "import {RunnableLambda} from '@langchain/core/run\n",
            "// the building blocks  \n",
            "const template = ChatPromptTemplate.fromMessages\n",
            "    ['system', 'You are a helpful assistant.'],  \n",
            "    ['human', '{question}'],  \n",
            "]) \n",
            "const model = new ChatOpenAI()  And the output:\n",
            "The preceding is a complete example of a chatbot, using a\n",
            "prompt and chat model. As you can see, it uses familiar\n",
            "Python syntax and supports any custom logic you might\n",
            "want to add in that function.\n",
            "On the other hand, if you want to enable streaming or\n",
            "async support, you’d have to modify your function to\n",
            "support it. F or example, streaming support can be added as\n",
            "follows, in Python ﬁrst:// combine them in a function  \n",
            "// RunnableLambda adds the same Runnable interfac\n",
            "const chatbot = RunnableLambda.from(async values \n",
            "  const prompt = await template.invoke(values)  \n",
            "  return await model.invoke(prompt)  \n",
            "}) \n",
            "// use it  \n",
            "const result = await chatbot.invoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})\n",
            "AIMessage(content=\"Hugging Face's `transformers` And in JS:\n",
            "And the output:@chain \n",
            "def chatbot(values):  \n",
            "    prompt = template.invoke(values)  \n",
            "    for token in model.invoke(prompt):  \n",
            "        yield token  \n",
            "for part in chatbot.stream({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "}): \n",
            "    print(part)\n",
            "const chatbot = RunnableLambda.from(async functio\n",
            "  const prompt = await template.invoke(values)  \n",
            "  for await (const token of await model.stream(pr\n",
            "      yield token  \n",
            "  } \n",
            "}) \n",
            "for await (const token of await chatbot.stream({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})) { \n",
            "    console.log(token)  \n",
            "}AIMessageChunk(content=\"Hugging\")  \n",
            "AIMessageChunk(content=\" Face's  \n",
            "AIMessageChunk(content=\" `transformers`  \n",
            "...\n",
            "So, either in JS or Python, you can enable streaming for\n",
            "your custom function by yielding the values you want to\n",
            "stream, and then calling it with stream .\n",
            "And for asynchronous execution you’d rewrite your\n",
            "function like this, in Python:\n",
            "This one applies to Python only as asynchronous execution\n",
            "is the only option in JavaScript.\n",
            "Declarative Composition@chain \n",
            "async def chatbot(values):  \n",
            "    prompt = await template.ainvoke(values)  \n",
            "    return await model.ainvoke(prompt)  \n",
            "result = await chatbot.ainvoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "}) \n",
            "# > AIMessage(content=\"Hugging Face's `transformeLangChain Expression Language (LCEL) is a declarative\n",
            "language  for composing LangChain components.\n",
            "LangChain compiles LCEL compositions to an optimized\n",
            "execution plan , with automatic parallelization, streaming,\n",
            "tracing, and async support.\n",
            "Let’s see the same example using LCEL, ﬁrst in Python:\n",
            "And in JS:from langchain_openai.chat_models import ChatOpen\n",
            "from langchain_core.prompts import ChatPromptTemp\n",
            "# the building blocks  \n",
            "template = ChatPromptTemplate.from_messages([  \n",
            "    ('system', 'You are a helpful assistant.'),  \n",
            "    ('human', '{question}'),  \n",
            "]) \n",
            "model = ChatOpenAI()  \n",
            "# combine them with the | operator  \n",
            "chatbot = template | model  \n",
            "# use it  \n",
            "result = chatbot.invoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})And the output:\n",
            "Crucially , the last line is the same between the two\n",
            "examples—that is, you use the function and the LCEL\n",
            "sequence in the same way , with invoke /stream /batch .import {ChatOpenAI} from '@langchain/openai'  \n",
            "import {ChatPromptTemplate} from '@langchain/core\n",
            "import {RunnableLambda} from '@langchain/core/run\n",
            "// the building blocks  \n",
            "const template = ChatPromptTemplate.fromMessages\n",
            "    ['system', 'You are a helpful assistant.'],  \n",
            "    ['human', '{question}'],  \n",
            "]) \n",
            "const model = new ChatOpenAI()  \n",
            "// combine them in a function  \n",
            "// RunnableLambda adds the same Runnable interfac\n",
            "const chatbot = template.pipe(model)  \n",
            "// use it  \n",
            "const result = await chatbot.invoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})\n",
            "AIMessage(content=\"Hugging Face's `transformers` And in this version, you don’t need to do anything else to\n",
            "use streaming, ﬁrst in Python:\n",
            "And in JS:\n",
            "And, for Python only , it’s the same for using asynchronous\n",
            "methods:chatbot = template | model  \n",
            "for part in chatbot.stream({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "}): \n",
            "    print(part)  \n",
            "    # > AIMessageChunk(content=\"Hugging\")  \n",
            "    # > AIMessageChunk(content=\" Face's  \n",
            "    # > AIMessageChunk(content=\" `transformers`  \n",
            "    # ...\n",
            "const chatbot = template.pipe(model)  \n",
            "for await (const token of await chatbot.stream({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})) { \n",
            "    console.log(token)  \n",
            "}Summary\n",
            "In this chapter , you’ve learned about the building blocks\n",
            "and key components necessary to build LLM applications\n",
            "using LangChain. LLM applications are essentially a chain\n",
            "consisting of the large language model to make predictions,\n",
            "the prompt instruction(s) to guide the model towards a\n",
            "desired output, and an optional output parser to transform\n",
            "the format of the model’s output.\n",
            "All LangChain components share the same interface with\n",
            "invoke , stream , and batch  methods to handle various\n",
            "inputs and outputs. They can either be combined and\n",
            "executed imperatively by calling them directly or\n",
            "declaratively using LangChain Expression Language\n",
            "(LCEL).\n",
            "The imperative approach is useful if you intend to write a\n",
            "lot of custom logic, whereas the declarative approach ischatbot = template | model  \n",
            "result = await chatbot.ainvoke({  \n",
            "    \"question\": \"Which model providers offer LLMs\n",
            "})useful for simply assembling existing components with\n",
            "limited customization.\n",
            "In the next chapter , you’ll learn how to provide external\n",
            "data to your AI chatbot as context , so that you can build an\n",
            "LLM application that enables you to “chat” with your data.Chapter 2. Indexing: Preparing\n",
            "Your Documents for LLMs\n",
            "A NOTE FOR EARLY RELEASE READERS\n",
            "With Early Release ebooks, you get books in their earliest\n",
            "form—the author’s raw and unedited content as they write\n",
            "—so you can take advantage of these technologies long\n",
            "before the oﬀicial release of these titles.\n",
            "This will be the 2nd chapter of the ﬁnal book. Please note\n",
            "that the GitHub repo will be made active later on.\n",
            "If you have comments about how we might improve the\n",
            "content and/or examples in this book, or if you notice\n",
            "missing material within this chapter , please reach out to\n",
            "the editor at ccollins@oreilly .com .\n",
            "In the previous chapter , you learned about the important\n",
            "building blocks used to create an LLM application using\n",
            "LangChain. Y ou also built a simple AI chatbot consisting of\n",
            "a prompt sent to the model and the output generated by\n",
            "the model. But there are major limitations to this simple\n",
            "chatbot.What if your use case requires knowledge the model wasn’t\n",
            "trained on? F or example, let’s say you want to use AI to ask\n",
            "questions about a company , but the information is stored in\n",
            "PDF documents, or in documents that are private to you or\n",
            "your company . While we’ve seen model providers enriching\n",
            "their training datasets to include more and more of the\n",
            "world’s public information (no matter what format it is\n",
            "stored in), two major limitations continue to exist in LLM’s\n",
            "knowledge corpus:\n",
            "Private data\n",
            "Information that isn’t publicly available is, by\n",
            "deﬁnition, not included in the training data of LLMs\n",
            "Current events\n",
            "Training an LLM is a costly and time consuming\n",
            "process that can span multiple years, with data\n",
            "gathering being one of the ﬁrst steps. This results in\n",
            "what is called the knowledge cutoﬀ , or a date beyond\n",
            "which the LLM has no knowledge of real world\n",
            "events, usually this would be the date the training set\n",
            "was ﬁnalized. This can be anywhere from a few\n",
            "months to a few years into the past, depending on the\n",
            "model in question.In either case, the model will most likely hallucinate and\n",
            "respond with inaccurate information. Adapting the prompt\n",
            "won’t resolve the issue either because it relies on the\n",
            "model’s current knowledge.\n",
            "The Goal: Picking Relevant\n",
            "Context for LLMs\n",
            "If the only private/current data you needed for your LLM\n",
            "use case was 1-2 pages of text, this chapter would be a lot\n",
            "shorter: all you’d need to make that information available\n",
            "to the LLM is to include that entire text in every single\n",
            "prompt you send to the model.\n",
            "The challenge in making data available to LLMs is ﬁrst and\n",
            "foremost a quantity problem. Y ou have more information\n",
            "than can ﬁt in each prompt you send to the LLM, so this is\n",
            "the problem to solve: which small subset of your large\n",
            "collection of text do you include each time you call the\n",
            "model? Or in other words, how do you pick (with the aid of\n",
            "the model) which text is most relevant to answer each\n",
            "question?In this chapter and the next, you’ll learn how to overcome\n",
            "this challenge in two steps:\n",
            "1. Indexing  your documents, that is, preprocessing them in\n",
            "a way where your application can easily ﬁnd the most\n",
            "relevant ones for each question\n",
            "2. Retrieving this external data from the index and using it\n",
            "as context  for the LLM to generate an accurate output\n",
            "based on your data.\n",
            "This chapter focuses on indexing, the ﬁrst step, which\n",
            "involves preprocessing your documents into a format that\n",
            "can be understood and searched with LLMs. But before we\n",
            "begin, let’s discuss why your documents require\n",
            "preprocessing.\n",
            "Let’s assume you would like to use LLMs to analyze the\n",
            "ﬁnancial performance and risks in T esla’s 2022 annual\n",
            "report , which is stored as text in PDF format. Y our goal is\n",
            "to be able to ask a question like What key risks did T esla\n",
            "face in 2022?  and get a human-like response based on\n",
            "context from the risk factors section of the document.\n",
            "Breaking it down, there are four key steps (visualized in\n",
            "Figure 2-1 ) that you’d need to take in order to achieve thisgoal:\n",
            "1. Extract the text from the document.\n",
            "2. Split the text into manageable chunks.\n",
            "3. Convert the text into numbers that computers can\n",
            "understand.\n",
            "4. Store these number representations of your text\n",
            "somewhere that makes it easy and fast to retrieve the\n",
            "relevant sections of your document to answer a given\n",
            "question.\n",
            "Figure 2-1. Four key steps to preprocess your documents for LLM usage.\n",
            "Figure 2-1  illustrates the ﬂow of this preprocessing and\n",
            "transformation of your documents, a process known as\n",
            "ingestion. Ingestion is simply the process of converting\n",
            "your documents into numbers that computers can\n",
            "understand and analyze, and storing them in a special type\n",
            "of database for eﬀicient retrieval. These numbers areformally known as embeddings , and this special type of\n",
            "database is known as a vector store. Let’s look a little more\n",
            "closely at what embeddings are and why they’re important,\n",
            "starting with something simpler than LLM-powered\n",
            "embeddings.\n",
            "Embeddings: Converting Text to\n",
            "Numbers\n",
            "Embedding  refers to representing text as a (long) sequence\n",
            "of numbers. This is a lossy representation—that is, you\n",
            "can’t recover the original text from these number\n",
            "sequences, so you usually store both the original text and\n",
            "this numeric representation.\n",
            "So, why bother? Because you gain the ﬂexibility and power\n",
            "that comes with working with numbers: you can do math on\n",
            "words! Let’s see why that’s exciting.\n",
            "Embeddings Before LLMs\n",
            "Long before LLMs, computer scientists were using\n",
            "embeddings—for instance, to enable full-text searchcapabilities in websites, or to classify emails as spam. Let’s\n",
            "see an example:\n",
            "1. Take these three sentences\n",
            "1. What a sunny day\n",
            "2. Such bright skies today\n",
            "3. I haven’t seen a sunny day in weeks\n",
            "2. List all unique words in them: what , a, sunny , day, such,\n",
            "bright , and so on\n",
            "3. For each sentence, go word by word and assign the\n",
            "number 0 if not present, 1 if used once in the sentence, 2\n",
            "if present twice, and so on.\n",
            "Table 2-1 shows the result.Table 2-1. Word embeddings for three sentences\n",
            "WordWhat a\n",
            "sunny daySuch\n",
            "bright\n",
            "skies\n",
            "todayI haven’t seen\n",
            "a sunny day in\n",
            "weeks\n",
            "what 1 0 0\n",
            "a 1 0 1\n",
            "sunny 1 0 1\n",
            "day 1 0 1\n",
            "such 0 1 0\n",
            "bright 0 1 0\n",
            "skies 0 1 0\n",
            "today 0 1 0\n",
            "I 0 0 1\n",
            "haven’t 0 0 1\n",
            "seen 0 0 1\n",
            "in 0 0 1WordWhat a\n",
            "sunny daySuch\n",
            "bright\n",
            "skies\n",
            "todayI haven’t seen\n",
            "a sunny day in\n",
            "weeks\n",
            "weeks 0 0 1\n",
            "In this model, the embedding for I haven’t seen a sunny day\n",
            "in weeks  is the sequence of numbers 0 1 1 1 0 0 0 0 1 1 1 1\n",
            "1. This is called the bag-of -words  model, and these\n",
            "embeddings are also called sparse embeddings  (or sparse\n",
            "vectors— vector  is another word for a sequence of\n",
            "numbers), because a lot of the numbers will be 0. Most\n",
            "English sentences use only a very small subset of all\n",
            "existing English words.\n",
            "You can successfully use this model for:\n",
            "Keyword search\n",
            "Finding which documents contain a given word or\n",
            "words.\n",
            "Classiﬁcation of documents\n",
            "You can calculate embeddings for a collection of\n",
            "examples previously labeled as email spam or notspam, average them out, and you obtain average word\n",
            "frequencies for each of the classes (spam or not\n",
            "spam). Then each new document is compared to those\n",
            "averages and classiﬁed accordingly\n",
            "The limitation here is that the model has no awareness of\n",
            "meaning, only of the actual words used. F or instance, the\n",
            "embeddings for sunny day  and bright skies  look very\n",
            "diﬀerent. In fact they have no words in common, even\n",
            "though we know they have similar meaning. Or , in the\n",
            "email classiﬁcation problem, a would-be spammer can trick\n",
            "the ﬁlter by replacing common “spam words” with their\n",
            "synonyms.\n",
            "In the next section we’ll see how semantic embeddings\n",
            "address this limitation by using numbers to represent the\n",
            "meaning of the text, instead of the exact words found in the\n",
            "text.\n",
            "LLM-based Embeddings\n",
            "We’re going to skip over all the ML developments that\n",
            "came in between and jump straight to LLM-based\n",
            "embeddings. Just know there was a gradual evolution fromthe simple method outlined in the previous section to the\n",
            "sophisticated method described in this one.\n",
            "You can think of embedding models as an oﬀshoot from the\n",
            "training process of LLMs. If you remember from the\n",
            "preface, the LLM training process (learning from vast\n",
            "amounts of written text) enables LLMs to complete a\n",
            "prompt (or input) with the most appropriate continuation (\n",
            "output). This capability stems from an understanding of the\n",
            "meaning of words and sentences in the context of the\n",
            "surrounding text, learned from how words are used\n",
            "together in the training texts. This understanding  of the\n",
            "meaning (or semantics) of the prompt can be extracted as a\n",
            "numeric representation (or embedding) of the input text,\n",
            "and can be used directly for some very interesting use\n",
            "cases too .\n",
            "In practice, most embedding models are trained for that\n",
            "purpose  alone, following somewhat similar architectures\n",
            "and training processes as LLMs, as that is more eﬀicient\n",
            "and results in higher-quality embeddings.\n",
            "An embedding model  then is an algorithm that takes a\n",
            "piece of text and outputs a numerical representation of its\n",
            "meaning—technically , a long list of ﬂoating point (decimal)numbers, usually somewhere between 100 and 2,000\n",
            "numbers, or dimensions . These are also called dense\n",
            "embeddings, as opposed to the sparse  embeddings of the\n",
            "previous section, as here usually all dimensions are\n",
            "diﬀerent from 0.\n",
            "TIP\n",
            "Diﬀerent models produce diﬀerent numbers, and diﬀerent sizes of lists.\n",
            "All of these are speciﬁc to each model, that is, even if the size of the\n",
            "lists matches, you cannot compare embeddings from diﬀerent models.\n",
            "Combining embeddings from diﬀerent models should always be\n",
            "avoided.\n",
            "Semantic Embeddings Explained\n",
            "Consider these three words: lion, pet, and dog. Intuitively ,\n",
            "which pair of these words share similar characteristics to\n",
            "each other at ﬁrst glance? The obvious answer is pet and\n",
            "dog. But computers do not have the ability to tap into this\n",
            "intuition or nuanced understanding of the English\n",
            "language. In order for a computer to diﬀerentiate between\n",
            "a pet, lion, or dog, you need to be able to translate them\n",
            "into the language of computers, which is numbers .Figure 2-2  illustrates converting each word into\n",
            "hypothetical number representations that retain their\n",
            "meaning.\n",
            "Figure 2-2. Semantic representations of words.\n",
            "Figure 2-2  shows each word alongside its corresponding\n",
            "semantic embedding. Note the numbers themselves have\n",
            "no particular meaning, but instead the sequences of\n",
            "numbers for two words (or sentences) that are close in\n",
            "meaning should be closer than those of unrelated words. As\n",
            "you can see, each number is a ﬂoating point value , and\n",
            "each of them represents a semantic dimension . Let’s see\n",
            "what we mean by closer :If we plot these vectors in a two dimensional space, it could\n",
            "look like Figure 2-3 .\n",
            " \n",
            "Figure 2-3. Plot of word vectors in a multidimensional space.\n",
            "Figure 2-3  shows the pet and dog plots are closer to each\n",
            "other in distance than the lion plot. W e can also observe\n",
            "that the angles between each plot varies depending on how\n",
            "similar they are. F or example, the words pet and lion have\n",
            "a wider angle between one another than the pet and dogdo, indicating more similarities shared by the latter word\n",
            "pairs. The narrower the angle, the closer the similarities.\n",
            "One eﬀective way to calculate the degree of similarity\n",
            "between two vectors in a multi-dimensional space is called\n",
            "cosine similarity . Cosine similarity  computes the dot\n",
            "product of vectors and divides it by the product of their\n",
            "magnitudes to output a number between -1 and 1, where 0\n",
            "means the vectors share no correlation, -1 means they are\n",
            "absolutely dissimilar , and 1 means they are absolutely\n",
            "similar . So, in the case of our three words here, the cosine\n",
            "similarity between pet and dog could be 0.75, but between\n",
            "pet and lion it might be 0.1.\n",
            "The ability to convert sentences into embeddings  that\n",
            "capture semantic meaning and then perform calculations to\n",
            "ﬁnd semantic similarities between diﬀerent sentences\n",
            "enables us to get an LLM to ﬁnd the most relevant\n",
            "documents to answer questions about a large body of text\n",
            "like our T esla PDF document. Now that you understand the\n",
            "big picture, let’s revisit the ﬁrst step of preprocessing your\n",
            "document.OTHER USES FOR EMBEDDINGS\n",
            "These sequences of numbers, vectors, have a number of\n",
            "interesting properties:\n",
            "As you learned earlier , if you think of a vector as\n",
            "describing a point in high-dimensional space, points that\n",
            "are closer together have more similar meanings, so a\n",
            "distance function can be used to measure similarity\n",
            "Groups of points close together can be said to be related,\n",
            "therefore a clustering algorithm can be used to identify\n",
            "topics (or clusters of points) and classify new inputs into\n",
            "one of those topics\n",
            "If you average out multiple embeddings, the average\n",
            "embedding can be said to represent the overall meaning\n",
            "of that group, ie you can embed a long document (for\n",
            "instance, this book) by\n",
            "a. Embedding each page separately , and then\n",
            "b. Taking the average of the embeddings of all pages as\n",
            "the book embedding\n",
            "You can “travel” the “meaning” space by using the\n",
            "elementary math operations of addition and subtraction:\n",
            "for instance, the operation king - man + woman = queen.\n",
            "If you take the meaning (or semantic embedding) of king,\n",
            "subtract the meaning of man, presumably you arrive atthe more abstract meaning of monarch , at which point, if\n",
            "you add the meaning of woman , you’ve arrived close to\n",
            "the meaning (or embedding) of the word queen .\n",
            "There are models that can produce embeddings for non-\n",
            "text content, for instance, images, videos, sounds, in\n",
            "addition to text. This enables, for instance, ﬁnding\n",
            "images that are most similar or relevant for a given\n",
            "sentence.\n",
            "We won’t explore all of these attributes in this book, but it’s\n",
            "useful to know they can be used for a number of\n",
            "applications  such as:\n",
            "Search\n",
            "Finding the most relevant documents for a new query\n",
            "Clustering\n",
            "Given a body of documents, divide them into groups\n",
            "(for instance, topics)\n",
            "Classiﬁcation\n",
            "Assigning a new document to a previously identiﬁed\n",
            "group or label (for instance, a topic)\n",
            "RecommendationGiven a document, surface similar documents\n",
            "Detecting anomalies\n",
            "Identify documents that are very dissimilar from\n",
            "previously seen ones\n",
            "We hope this leaves you with some intuition that\n",
            "embeddings are quite versatile and can be put to good use\n",
            "in your future projects.\n",
            "Converting Your Documents into\n",
            "Text\n",
            "As mentioned at the beginning of the chapter , the ﬁrst step\n",
            "in preprocessing your document is to convert it to text. In\n",
            "order to achieve this, you would need to build logic to parse\n",
            "and extract the document with minimal loss of quality .\n",
            "Fortunately , LangChain provides document loaders that\n",
            "handle the parsing logic and enable you to “load” data from\n",
            "various sources into a Document  class which consists of\n",
            "text and associated metadata.For example, consider a simple .txt ﬁle. Y ou can simply\n",
            "import a LangChain T extLoader class to extract the text,\n",
            "like this, ﬁrst in Python:\n",
            "Now in JS:\n",
            "And the output\n",
            "The code block above assumes you have a ﬁle named\n",
            "test.txt  in your current directory . Usage of all\n",
            "LangChain document loaders follows a similar pattern: \n",
            "from langchain_community.document_loaders import \n",
            "loader = TextLoader(\"./test.txt\")  \n",
            "loader.load()\n",
            " \n",
            "import { TextLoader } from \"langchain/document_lo\n",
            "const loader = new TextLoader(\"./test.txt\");  \n",
            "const docs = await loader.load();\n",
            " \n",
            "[Document(page_content='text content \\n', metadat1. You start by picking the loader for your type of document\n",
            "from the long list of integrations here\n",
            "2. You create an instance of the loader in question, along\n",
            "with any parameters to conﬁgure it, including the\n",
            "location of your documents (usually a ﬁlesystem path or\n",
            "web address)\n",
            "3. You load the documents by calling load() , which\n",
            "returns a list of documents ready to pass to the next\n",
            "stage (more on that soon).\n",
            "Aside from .txt ﬁles, LangChain provides document loaders\n",
            "for other popular ﬁle types including .csv, .json , and\n",
            "Markdown, alongside integrations with popular platforms\n",
            "such as Slack and Notion.\n",
            "For example, you can use W ebBaseLoader to load HTML\n",
            "from web URLs and parse it to text. First in Python:\n",
            "And in JS: \n",
            "from langchain_community.document_loaders import \n",
            "loader = WebBaseLoader(\"https://www.langchain.com\n",
            "loader.load()In the case of our T esla PDF use case, we can utilize a\n",
            "LangChain’s PDF Loader to extract text from the PDF\n",
            "document. First in Python:\n",
            "And in JS: \n",
            "// install cheerio: npm install cheerio  \n",
            "import { CheerioWebBaseLoader } from \"@langchain/\n",
            "const loader = new CheerioWebBaseLoader(\"https://\n",
            "const docs = await loader.load();\n",
            " \n",
            "# install the pdf parsing library  \n",
            "!pip install pypdf  \n",
            "from langchain_community.document_loaders import \n",
            "loader = PyPDFLoader(\"./test.pdf\")  \n",
            "pages = loader.load()\n",
            " \n",
            "// install the pdf parsing library: npm install p\n",
            "import { PDFLoader } from \"langchain/document_loa\n",
            "const loader = new PDFLoader(\"./test.pdf\");  \n",
            "const docs = await loader.load();The text has been extracted from the PDF document and\n",
            "stored in the Document  class. But there’s a problem. The\n",
            "loaded document is over 100,000 characters long, so it\n",
            "won’t ﬁt into the context window of the vast majority of\n",
            "LLMs or Embedding models. In order to overcome this\n",
            "limitation, we need to split the Document  into manageable\n",
            "chunks of text that we can later convert into embeddings\n",
            "and semantically search, bringing us to step 2.\n",
            "TIP\n",
            "LLMs and Embedding models are designed with a hard limit on the size\n",
            "of input and output text they can handle. This limit is usually called\n",
            "context window, and usually applies to the combination of input and\n",
            "output, that is, if the context window is 100 (we’ll talk about units in a\n",
            "second), and your input measures 90, the output can be at most of\n",
            "length 10. Context windows are usually measured in number of tokens,\n",
            "for instance 8,192 tokens. Tokens, as mentioned in the Preface, are a\n",
            "representation of text as numbers, with each token usually covering\n",
            "between 3-4 characters of English text.\n",
            "Splitting Your Text Into Chunks\n",
            "At ﬁrst glance it may seem straightforward to split a large\n",
            "body of text into chunks, but keeping semantically  related(related by meaning) chunks of text together is a complex\n",
            "process. T o make it easier to split large documents into\n",
            "small, but still meaningful, pieces of text, LangChain\n",
            "provides RecursiveCharacterT extSplitter , which does the\n",
            "following:\n",
            "1. Take a list of separators, in order of importance. By\n",
            "default these are\n",
            "1. The paragraph separator: \\n\\n\n",
            "2. The line separator: \\n\n",
            "3. The word separator: space character\n",
            "2. To respect the given chunk size, for instance, 1,000\n",
            "characters, start by splitting up paragraphs.\n",
            "3. For any paragraph longer than the desired chunk size,\n",
            "split by the next separator , that is lines. Continue until all\n",
            "chunks are smaller than the desired length, or there are\n",
            "no additional separators to try\n",
            "4. Emit each chunk as a Document , with the metadata of\n",
            "the original document passed in, and additional\n",
            "information about the position in the original document.\n",
            "These LangChain text splitters handle various data formats,\n",
            "including HTML, code, JSON , Markdown, text, and more.\n",
            "Let’s see an example, ﬁrst in Python:And in JS:\n",
            "In the preceding code, the documents created by the\n",
            "document loader are split into chunks of 1000 characters \n",
            "from langchain_text_splitters import CharacterTex\n",
            "loader = TextLoader(\"./test.txt\") # or any other \n",
            "docs = loader.load()  \n",
            "splitter = RecursiveCharacterTextSplitter(  \n",
            "    chunk_size=1000,  \n",
            "    chunk_overlap=200,  \n",
            ") \n",
            "splitted_docs = splitter.split_documents(docs)\n",
            " \n",
            "import { TextLoader } from \"langchain/document_lo\n",
            "import { RecursiveCharacterTextSplitter } from \"@\n",
            "const loader = new TextLoader(\"./test.txt\"); // o\n",
            "const docs = await loader.load();  \n",
            "const splitter = new RecursiveCharacterTextSplitt\n",
            "  chunkSize: 1000,  \n",
            "  chunkOverlap: 200,  \n",
            "}); \n",
            "const splittedDocs = await splitter.splitDocumenteach, with some overlap between chunks of 200 characters\n",
            "to maintain some context. The result is also a list of\n",
            "documents, where each document is up to 1,000 characters\n",
            "in length, split along the natural divisions of written text,\n",
            "that is paragraphs, new lines and ﬁnally , words. This uses\n",
            "the structure of the text to keep each chunk a consistent,\n",
            "readable snippet of text.\n",
            "RecursiveCharacterT extSplitter can also be used to split\n",
            "code languages and Markdown into semantic chunks. This\n",
            "is done by using keywords speciﬁc to each language as the\n",
            "separators, which ensures, for instance, the body of each\n",
            "function is kept in the same chunk, instead of split between\n",
            "several. Usually , as programming languages have more\n",
            "structure than written text, there’s less need to use overlap\n",
            "between the chunks. LangChain contains separators for a\n",
            "number of popular languages, such as Python, JS ,\n",
            "Markdown, HTML, and many more. Let’s see an example,\n",
            "ﬁrst in Python:\n",
            " \n",
            "from langchain_text_splitters import (  \n",
            "    Language,  \n",
            "    RecursiveCharacterTextSplitter,  \n",
            ") And in JS:PYTHON_CODE = \"\"\"  \n",
            "def hello_world():  \n",
            "    print(\"Hello, World!\")  \n",
            "# Call the function  \n",
            "hello_world()  \n",
            "\"\"\" \n",
            "python_splitter = RecursiveCharacterTextSplitter\n",
            "    language=Language.PYTHON, chunk_size=50, chun\n",
            ") \n",
            "python_docs = python_splitter.create_documents([P\n",
            " \n",
            "import { RecursiveCharacterTextSplitter } from \"@\n",
            "const PYTHON_CODE = `  \n",
            "def hello_world():  \n",
            "    print(\"Hello, World!\")  \n",
            "# Call the function  \n",
            "hello_world()  \n",
            "`; \n",
            "const pythonSplitter = RecursiveCharacterTextSpli\n",
            "  chunkSize: 50,  \n",
            "  chunkOverlap: 0,  \n",
            "}); \n",
            "const pythonDocs = await pythonSplitter.createDocAnd the output:\n",
            "Notice how we’re still using\n",
            "RecursiveCharacterTextSplitter  as above, but now\n",
            "we’re creating an instance of it for a speciﬁc language,\n",
            "using the from_language  method. This one accepts the\n",
            "name of the language, and the usual parameters for chunk\n",
            "size, and so on. Also notice we are now using the method\n",
            "create_documents , which accepts a list of strings, rather\n",
            "than the list of documents we had before. This method is\n",
            "useful when the text you want to split doesn’t come from a\n",
            "document loader , so you have only the raw text strings.\n",
            "You can also use the optional second argument to\n",
            "create_documents  to pass a list of metadata to associate\n",
            "with each text string. This metadata list should have the\n",
            "same length as the list of strings, and will be used to\n",
            "populate the metadata ﬁeld of each Document  returned.\n",
            "Let’s see an example for Markdown text, using the\n",
            "metadata argument as well. First in Python: \n",
            "[Document(page_content='def hello_world():\\n    p\n",
            " Document(page_content='# Call the function\\nhellAnd in JS: \n",
            "markdown_text = \"\"\"  \n",
            "# LangChain  \n",
            "Building applications with LLMs through composabi\n",
            "## Quick Install  \n",
            "```bash \n",
            "pip install langchain  \n",
            "``` \n",
            "As an open-source project in a rapidly developing\n",
            "\"\"\" \n",
            "md_splitter = RecursiveCharacterTextSplitter.from\n",
            "    language=Language.MARKDOWN, chunk_size=60, ch\n",
            ") \n",
            "md_docs = md_splitter.create_documents([markdown_\n",
            " \n",
            "const markdownText = `  \n",
            "# LangChain  \n",
            "Building applications with LLMs through composabi\n",
            "## Quick Install  \n",
            "\\`\\`\\`bash  \n",
            "pip install langchain  \n",
            "\\`\\`\\` \n",
            "As an open-source project in a rapidly developing\n",
            "`; And the output:\n",
            "Notice two things:\n",
            "The text is split along the natural stopping points in the\n",
            "Markdown document, for instance the heading goes into\n",
            "one chunk, the line of text under it in a separate chunk,\n",
            "and so on.\n",
            "The metadata we passed in the second argument is\n",
            "attached to each resulting document, which allows you toconst mdSplitter = RecursiveCharacterTextSplitter\n",
            "  chunkSize: 60,  \n",
            "  chunkOverlap: 0,  \n",
            "}); \n",
            "const mdDocs = await mdSplitter.createDocuments(\n",
            " \n",
            "[Document(page_content='# LangChain', metadata={\"\n",
            " Document(page_content=' Building applications wi\n",
            " Document(page_content='## Quick Install\\n\\n```ba\n",
            " Document(page_content='pip install langchain', m\n",
            " Document(page_content='```', metadata={\"source\"\n",
            " Document(page_content='As an open-source project\n",
            " Document(page_content='are extremely open to contrack, for instance, where the document came from, and\n",
            "where you can go to see the original.\n",
            "Generating Text Embeddings\n",
            "LangChain also has an Embeddings  class designed to\n",
            "interface with text embedding models, including OpenAI,\n",
            "Cohere, and Hugging F ace, and generate vector\n",
            "representations of text. This class provides two methods:\n",
            "one for embedding documents and one for embedding a\n",
            "query . The former takes as input multiple texts, while the\n",
            "latter takes a single text.\n",
            "Here’s an example of embedding a document using\n",
            "OpenAI’s embedding model , ﬁrst in Python:\n",
            " \n",
            "from langchain_openai import OpenAIEmbeddings  \n",
            "model = OpenAIEmbeddings()  \n",
            "embeddings = model.embed_documents([  \n",
            "    \"Hi there!\",  \n",
            "    \"Oh, hello!\",  \n",
            "    \"What's your name?\",  \n",
            "    \"My friends call me World\",      \"Hello World!\"  \n",
            "])\n",
            "And in JS:\n",
            "And the output: \n",
            "import { OpenAIEmbeddings } from \"@langchain/open\n",
            "const model = new OpenAIEmbeddings();  \n",
            "const embeddings = await embeddings.embedDocument\n",
            "    \"Hi there!\",  \n",
            "    \"Oh, hello!\",  \n",
            "    \"What's your name?\",  \n",
            "    \"My friends call me World\",  \n",
            "    \"Hello World!\"  \n",
            "]);\n",
            " \n",
            "[ \n",
            "  [ \n",
            "    -0.004845875,   0.004899438,  -0.016358767,  \n",
            "      0.012571548,  -0.019156644,   0.009036391, \n",
            "      0.022861943,   0.010321903,  -0.023479493, \n",
            "    0.0026371893,   0.025206111,  -0.012048521,  \n",
            "    -0.010580265,  -0.003509951,   0.004070787,  Notice you can embed multiple documents at the same\n",
            "time, you should prefer this to embedding them one at a\n",
            "time, as it will be more eﬀicient (due to how these models\n",
            "are constructed). Y ou get back a list of lists of numbers,\n",
            "each inner list is a vector , or embedding, as explained in an\n",
            "earlier section.\n",
            "Now let’s see an end-to-end example using the three\n",
            "capabilities we’ve seen so far\n",
            "Document loaders, to convert any document to plain text\n",
            "Text splitters, to split each large document into many\n",
            "smaller ones    ... 1511 more items  \n",
            "  ] \n",
            "  [ \n",
            "      -0.009446913,  -0.013253193,   0.013174579,\n",
            "      0.0077763423,    -0.0260478, -0.0114384955,\n",
            "      0.041797023,    0.01787183,    0.00552271, \n",
            "      -0.01542166,   0.033752076,   0.006112323, \n",
            "      -0.006623321,   0.016116094, -0.0061090477,\n",
            "    ... 1511 more items  \n",
            "  ] \n",
            "  ... 3 more items  \n",
            "]Embeddings models, to create a numeric representation\n",
            "of the meaning of each split\n",
            "First in Python:\n",
            " \n",
            "from langchain_community.document_loaders import \n",
            "from langchain_text_splitters import RecursiveCha\n",
            "from langchain_openai import OpenAIEmbeddings  \n",
            "## Load the document  \n",
            "loader = TextLoader(\"./test.txt\")  \n",
            "doc = loader.load()  \n",
            "\"\"\" \n",
            "[ \n",
            "    Document(page_content='Document loaders\\n\\nUs\n",
            "] \n",
            "\"\"\" \n",
            "## Split the document  \n",
            "text_splitter = RecursiveCharacterTextSplitter(  \n",
            "    chunk_size=1000,  \n",
            "    chunk_overlap=20,  \n",
            ") \n",
            "chunks = text_splitter.split_documents(doc)  \n",
            "## Generate embeddings  \n",
            "model = OpenAIEmbeddings()  \n",
            "embeddings = embeddings_model.embed_documents(chu\n",
            "\"\"\" \n",
            "[[0.0053587136790156364,  And in JS: -0.0004999046213924885,  \n",
            " 0.038883671164512634,  \n",
            " -0.003001077566295862,  \n",
            " -0.00900818221271038, ...], ...]  \n",
            "\"\"\"\n",
            " \n",
            "import { TextLoader } from \"langchain/document_lo\n",
            "import { RecursiveCharacterTextSplitter } from \"@\n",
            "import { OpenAIEmbeddings } from \"@langchain/open\n",
            "// Load the document  \n",
            "const loader = new TextLoader(\"./test.txt\");  \n",
            "const docs = await loader.load();  \n",
            "// Split the document  \n",
            "const splitter = new RecursiveCharacterTextSplitt\n",
            "  chunkSize: 1000,  \n",
            "  chunkOverlap: 200,  \n",
            "}); \n",
            "const chunks = await splitter.splitDocuments(docs\n",
            "// Generate embeddings  \n",
            "const model = new OpenAIEmbeddings();  \n",
            "const embeddings = await embeddings.embedDocumentOnce you’ve generated embeddings from your documents,\n",
            "the next step is to store them in a special database known\n",
            "as a V ector Store.\n",
            "Storing Embeddings in a Vector\n",
            "Store\n",
            "Earlier in this chapter , we discussed the cosine similarity\n",
            "calculation to measure the similarity between vectors in a\n",
            "vector space. A vector store  is a database designed to store\n",
            "vectors and perform complex calculations like cosine\n",
            "similarity eﬀiciently and quickly .\n",
            "Unlike traditional databases that specialize in storing\n",
            "structured data (such as JSON documents or data\n",
            "conforming to the schema of a relational database), vector\n",
            "stores handle unstructured data, including text and images.\n",
            "Like traditional databases, vector stores are capable of\n",
            "performing create-read-update-delete (CRUD) and search\n",
            "operations.\n",
            "Vector stores unlock a wide variety of use cases, including\n",
            "scalable applications that utilize AI to answer questions\n",
            "about large documents, as illustrated in Figure 2-4 .Figure 2-4. Loading, embedding, storing, and retrieving relevant docs from a\n",
            "vector store.\n",
            "Figure 2-4  illustrates how document embeddings are\n",
            "inserted into the vector store and how later , when a query\n",
            "is sent, similar embeddings are retrieved from the vector\n",
            "store.\n",
            "Currently , there is an abundance of vector store providers\n",
            "to choose from, each specializing in diﬀerent capabilities.\n",
            "Your selection should depend on the critical requirements\n",
            "of your application, including multi-tenancy , metadata\n",
            "ﬁltering capabilities, performance, cost, and scalability .\n",
            "Although vector stores are niche databases built to manage\n",
            "vector data, there are a few disadvantages working with\n",
            "them:\n",
            "Most vector stores are relatively new and may not stand\n",
            "the test of time.\n",
            "Managing and optimizing vector stores can present a\n",
            "relatively steep learning curve.Managing a separate database adds complexity to your\n",
            "application and may drain valuable resources.\n",
            "Fortunately , vector store capabilities have recently been\n",
            "extended to P ostgreSQL (a popular open-source relational\n",
            "database) via the pgvector extension. This enables you to\n",
            "use the same database you’re already familiar with, to\n",
            "power both your transactional tables (for instance your\n",
            "users table) as well as your vector search tables.\n",
            "Getting set up with Pgvector\n",
            "To use P ostgres and Pgvector you’ll need to follow a few\n",
            "setup steps:\n",
            "1. Ensure you have Docker installed on your computer , see\n",
            "instructions for your operating system here.\n",
            "2. Run the following command in your terminal, it will\n",
            "launch a P ostgres instance in your computer running on\n",
            "port 6024.\n",
            "3. Save the connection string to use in your code, we’ll\n",
            "need it later:\n",
            "postgresql+psycopg://langchain:langchain@localho\n",
            "st:6024/langchainWorking with Vector Stores\n",
            "Picking up where we left oﬀ in the previous section on\n",
            "Embeddings, now let’s see an example of loading, splitting,\n",
            "embedding, and storing a document in Pgvector , ﬁrst in\n",
            "Python:\n",
            "And in JS: \n",
            "docker run -e POSTGRES_USER=langchain -e POSTGRES\n",
            "from langchain_community.document_loaders import \n",
            "from langchain_openai import OpenAIEmbeddings  \n",
            "from langchain_text_splitters import RecursiveCha\n",
            "from langchain_postgres.vectorstores import PGVec\n",
            "# Load the document, split it into chunks  \n",
            "raw_documents = TextLoader('./test.txt').load()  \n",
            "text_splitter = RecursiveCharacterTextSplitter(ch\n",
            "documents = text_splitter.split_documents(raw_doc\n",
            "# embed each chunk and insert it into the vector \n",
            "model = OpenAIEmbeddings()  \n",
            "connection = 'postgresql+psycopg://langchain:lang\n",
            "db = PGVector.from_documents(documents, model, coNotice how we reuse the code from the previous sections,\n",
            "to ﬁrst load the documents with the loader , and then split\n",
            "them into smaller chunks. Then we instantiate the\n",
            "Embeddings model we want to use, in this case OpenAI’s. \n",
            "import { TextLoader } from \"langchain/document_lo\n",
            "import { RecursiveCharacterTextSplitter } from \"@\n",
            "import { OpenAIEmbeddings } from \"@langchain/open\n",
            "import { PGVectorStore } from \"@langchain/communi\n",
            "// Load the document, split it into chunks  \n",
            "const loader = new TextLoader(\"./test.txt\");  \n",
            "const raw_docs = await loader.load();  \n",
            "const splitter = new RecursiveCharacterTextSplitt\n",
            "  chunkSize: 1000,  \n",
            "  chunkOverlap: 200,  \n",
            "}); \n",
            "const docs = await splitter.splitDocuments(docs)  \n",
            "// embed each chunk and insert it into the vector\n",
            "const model = new OpenAIEmbeddings();  \n",
            "const db = await PGVectorStore.fromDocuments(docs\n",
            "  postgresConnectionOptions: {  \n",
            "    connectionString: 'postgresql://langchain:lan\n",
            "  } \n",
            "})Note you could use any other embeddings model supported\n",
            "by LangChain here.\n",
            "Then we have a new line of code, which creates a vector\n",
            "store given documents, the embeddings model, and a\n",
            "connection string. This will do a few things:\n",
            "Establish a connection to the P ostgres instance running\n",
            "in your computer (see the setup section just before this\n",
            "one\n",
            "Run any setup necessary , such as creating tables to hold\n",
            "your documents and vectors, if this is the ﬁrst time\n",
            "you’re running it\n",
            "Create embeddings for each document you passed in,\n",
            "using the model you chose\n",
            "Store the embeddings, the document’s metadata, and the\n",
            "document’s text content in P ostgres, ready to be\n",
            "searched.\n",
            "Let’s see what it looks like to search documents, ﬁrst in\n",
            "Python:\n",
            " \n",
            "db.similarity_search(\"query\", k=4)And in JS:\n",
            "This method will ﬁnd the most relevant documents (which\n",
            "you previously indexed as above), by following this process:\n",
            "The search query , in this case the word query , will be\n",
            "sent to the embeddings model to retrieve its embedding\n",
            "Then it will run a query on P ostgres to ﬁnd the N (in this\n",
            "case 4) previously stored embeddings that are most\n",
            "similar to your query\n",
            "Finally it will fetch the text content and metadata that\n",
            "relates to each of those embeddings\n",
            "And return a list of Document  sorted by how similar they\n",
            "are to the query , the most similar ﬁrst, the second most\n",
            "similar after , and so on,\n",
            "You can also add more documents to an existing database,\n",
            "let’s see an example, ﬁrst in Python: \n",
            "await pgvectorStore.similaritySearch(\"query\", 4);\n",
            " \n",
            "db.add_documents([  \n",
            "  Document(  And in JS:\n",
            "The add_documents  method we’re using here will follow a\n",
            "similar process to fromDocuments\n",
            "Create embeddings for each document you passed in,\n",
            "using the model you chose      page_content=\"there are cats in the pond\",  \n",
            "      metadata={\"location\": \"pond\", \"topic\": \"ani\n",
            "  ), \n",
            "  Document(  \n",
            "      page_content=\"ducks are also found in the p\n",
            "      metadata={\"location\": \"pond\", \"topic\": \"ani\n",
            "  ), \n",
            "], ids=[1, 2])\n",
            " \n",
            "await db.addDocuments([  \n",
            "  { pageContent: \"there are cats in the pond\", me\n",
            "  { pageContent: \"ducks are also found in the pon\n",
            "], { \n",
            "  ids: [1, 2]  \n",
            "});Store the embeddings, the document’s metadata, and the\n",
            "document’s text content in P ostgres, ready to be\n",
            "searched.\n",
            "In this example we are using the optional ids argument to\n",
            "assign identiﬁers to each document, which allows us to\n",
            "update or delete them later .\n",
            "Let’s see an example of the delete operation, ﬁrst in\n",
            "Python:\n",
            " \n",
            "db.delete(ids=[2])\n",
            "And in JS:\n",
            " \n",
            "await db.delete({ ids: [2] })\n",
            "This removes the document we had previously inserted\n",
            "with id 2. Now let’s see how to do this in a more systematic\n",
            "way.Tracking Changes to your\n",
            "Documents\n",
            "One of the key challenges with working with vector stores\n",
            "is working with data that regularly changes, because\n",
            "changes mean re-indexing. And re-indexing can lead to\n",
            "costly re-computations of embeddings and duplications of\n",
            "pre-existing content.\n",
            "Fortunately , LangChain provides an Indexing API to make it\n",
            "easy to keep your documents in sync with your vector\n",
            "store.The API utilizes a class ( RecordManager ) to keep\n",
            "track of document writes into the vector store. When\n",
            "indexing content, hashes are computed for each document,\n",
            "and the following information is stored in RecordManager :\n",
            "The document hash (hash of both page content and\n",
            "metadata)\n",
            "Write time\n",
            "The source id (each document should include information\n",
            "in its metadata to determine the ultimate source of this\n",
            "document).In addition, the Indexing API provides cleanup modes to\n",
            "help you decide how to delete existing documents in the\n",
            "vector store. F or example, If you’ve made changes to how\n",
            "documents are processed before insertion or source\n",
            "documents have changed, you may want to remove any\n",
            "existing documents that come from the same source as the\n",
            "new documents being indexed. If some source documents\n",
            "have been deleted, you’ll want to delete all existing\n",
            "documents in the vector store and replace them with the\n",
            "re-indexed documents.\n",
            "The modes are as follows:\n",
            "None  mode does not do any automatic cleanup, allowing\n",
            "the user to manually do cleanup of old content.\n",
            "Incremental  and full  modes delete previous versions\n",
            "of the content if the content of the source document or\n",
            "derived documents has changed.\n",
            "Full  mode will additionally delete any documents not\n",
            "included in documents currently being indexed.\n",
            "Here’s an example of the use of the Indexing API with\n",
            "Postgres database set up as a record manager , ﬁrst in\n",
            "Python:And in JS: \n",
            "from langchain.indexes import SQLRecordManager, i\n",
            "record_manager = SQLRecordManager(  \n",
            "    namespace, db_url=\"postgresql+psycopg://langc\n",
            ") \n",
            "# Create the schema if it doesn't exist  \n",
            "record_manager.create_schema()  \n",
            "index( \n",
            "    [doc1Updated, doc2],  \n",
            "    record_manager,  \n",
            "    vectorstore,  \n",
            "    cleanup='incremental',  \n",
            ")\n",
            " \n",
            "import { PostgresRecordManager } from \"@langchain\n",
            "import { index } from \"langchain/indexes\";  \n",
            "const recordManager = new PostgresRecordManager(  \n",
            "  \"test_namespace\", {  \n",
            "  postgresConnectionOptions: {  \n",
            "    connectionString: 'postgresql://langchain:lan\n",
            "  } \n",
            "}); \n",
            "// Create the schema if it doesn't exist  \n",
            "await recordManager.createSchema();  First, you create a record manager , which keeps track of\n",
            "which documents have been indexed before. Then you use\n",
            "the index  function to synchronize your vector store with\n",
            "the new list of documents. In this example we’re using the\n",
            "incremental mode, so any documents that have the same id\n",
            "as previous ones will be replaced with the new version.\n",
            "Summary\n",
            "In this chapter , you’ve learned how to prepare and\n",
            "preprocess your documents for your LLM application using\n",
            "various LangChain’s modules. The document loaders\n",
            "enable you to extract text from your data source, text\n",
            "splitters help you split your document into semantically\n",
            "similar chunks, and the embeddings models convert yourawait index({  \n",
            "  docsSource: [doc1Updated, doc2],  \n",
            "  recordManager,  \n",
            "  vectorStore,  \n",
            "  options: {  \n",
            "    cleanup: 'incremental',  \n",
            "  }, \n",
            "})text into vector representations of their meaning. Finally ,\n",
            "vector stores allow you to perform CRUD operations on\n",
            "these embeddings alongside complex calculations to\n",
            "compute semantically similar chunks of text.\n",
            "In the next chapter , you’ll learn how to eﬀiciently retrieve\n",
            "the most similar chunks of documents from your vector\n",
            "store based on your query , provide it as context  the model\n",
            "can see, and then generate an accurate output.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f95d76d4",
        "outputId": "937c0c11-1f25-4e2b-da53-a28d44612459"
      },
      "source": [
        "def chunk_text(text, chunk_size):\n",
        "    \"\"\"Splits text into chunks of a specified size.\"\"\"\n",
        "    words = text.split()\n",
        "    for i in range(0, len(words), chunk_size):\n",
        "        yield \" \".join(words[i:i + chunk_size])\n",
        "\n",
        "# Define a chunk size (you might need to experiment with this value)\n",
        "chunk_size = 500  # Example chunk size\n",
        "\n",
        "# Split the full text into chunks\n",
        "text_chunks = list(chunk_text(full_text, chunk_size))\n",
        "\n",
        "print(f\"Split text into {len(text_chunks)} chunks.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split text into 32 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "415eba30",
        "outputId": "1bb9d41b-c5fb-4b87-ad70-177c83186ce6"
      },
      "source": [
        "summaries = []\n",
        "for i, chunk in enumerate(text_chunks):\n",
        "    print(f\"Summarizing chunk {i+1}/{len(text_chunks)}...\")\n",
        "    try:\n",
        "        result = client.summarization(\n",
        "            chunk,\n",
        "            model=\"facebook/bart-large-cnn\",\n",
        "        )\n",
        "        summaries.append(result.summary_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error summarizing chunk {i+1}: {e}\")\n",
        "        # Optionally, you can append the original chunk if summarization fails\n",
        "        # summaries.append(chunk)\n",
        "\n",
        "# Combine the summaries\n",
        "combined_summary = \" \".join(summaries)\n",
        "\n",
        "print(\"\\nCombined Summary:\")\n",
        "print(combined_summary)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing chunk 1/32...\n",
            "Summarizing chunk 2/32...\n",
            "Summarizing chunk 3/32...\n",
            "Summarizing chunk 4/32...\n",
            "Summarizing chunk 5/32...\n",
            "Summarizing chunk 6/32...\n",
            "Summarizing chunk 7/32...\n",
            "Summarizing chunk 8/32...\n",
            "Summarizing chunk 9/32...\n",
            "Summarizing chunk 10/32...\n",
            "Summarizing chunk 11/32...\n",
            "Summarizing chunk 12/32...\n",
            "Summarizing chunk 13/32...\n",
            "Summarizing chunk 14/32...\n",
            "Summarizing chunk 15/32...\n",
            "Summarizing chunk 16/32...\n",
            "Summarizing chunk 17/32...\n",
            "Summarizing chunk 18/32...\n",
            "Summarizing chunk 19/32...\n",
            "Summarizing chunk 20/32...\n",
            "Summarizing chunk 21/32...\n",
            "Summarizing chunk 22/32...\n",
            "Summarizing chunk 23/32...\n",
            "Summarizing chunk 24/32...\n",
            "Summarizing chunk 25/32...\n",
            "Summarizing chunk 26/32...\n",
            "Summarizing chunk 27/32...\n",
            "Summarizing chunk 28/32...\n",
            "Summarizing chunk 29/32...\n",
            "Error summarizing chunk 29: (Request ID: Root=1-68a9d952-1c4a970802c36dbf526925ad;31c676fb-8169-4807-bf78-cd313245ba54)\n",
            "\n",
            "Bad request:\n",
            "index out of range in self\n",
            "Summarizing chunk 30/32...\n",
            "Summarizing chunk 31/32...\n",
            "Summarizing chunk 32/32...\n",
            "\n",
            "Combined Summary:\n",
            "Learn LangChain Build an AI Chatbot Trained on Y our Data With Early Release ebooks, you get books in their earliest form. You can take advantage of these technologies long before the oﬀicial release of these titles. See http://oreilly .com/catalog/errata.csp? isbn=9781098167288. ChatGPT is a chatbot experiencepowered by an instruction and dialogue-tuned version of OpenAI’s GPT -3.5 family of large language models (LLMs) Building LLM applications with or without LangChain requires the use of an LLM (read on for explanations for all these concepts) On October 22, 2022, Harrison Chase published the ﬁrst commit on GitHub for the LangChain open source library. By the end of 2023, competing LLMs emerged, including Anthropic’s Claude and Google's Bard. Thousands of successful startups and major enterprises have incorporated generative AI APIs to build applications. Because of the prevalence of English in the training data, most models are better at English than they are at other languages. The driving engine behind LLMs’ predictive power is known as the transformer neural network architecture. Transformers are designed to understand the context of each word in a sentence by considering it in relation to every other word. The instructions and input text you provide to the model is called a prompt. Prompting can have a signiﬁcant impacton the quality of output from the LLM. There are several best practices for prompt design or prompt engineering, including providing clear and concise instructions with contextual examples.  instruction-tuning has been key to broadening the number of people that can build applications with LLMs. This has made it a lot easier to use these models to power chatbot interfaces. More specialized models have now been produced, in other words, LLMs tuned speciﬁcally for dialogue or chat tasks. This book explains how to use LangChain to get LLMs to do what you have in mind. We use the term LLM to refer to instruction-tuned LLMs, and for chat model we mean dialogue-instructed LLMs. The main task of the software engineer working with LLMs is not to train an LLM, but to take an existing LLM and work out how to get it to accomplish the task you need. OpenAI’s gpt-3.5-turbo-instruct-0914 can be used to answer questions. The question is: How old was the 30th president of the United States, Calvin Coolidge? The LLM produces a sequence of steps you can go through when trying to answer this question. Unfortunately, the steps are very reasonable, but the output is still incorrect, given it got some facts wrong. LangChain was the ﬁrst library to provide LLM and prompting building blocks and the tooling to reliably combine them into larger applications. LangChain has amassed over 7 million monthly downloads, 82,000 GitHub stars, and the largest developer community in generative AI ( 72,000+ strong ). LangChain is a library that lets you build AI applications using Python and JavaScript. It provides simple abstractions for each major prompting technique. LangChain provides integrations with the major LLM providers, both commercial (OpenAI, Anthropic, Google, and more) and open-source ( Llama, Gemma, and others) Programming with LLMs is so exciting to me because it expands the set of things I can build. It makes previously hard things easy (for example, extracting relevant numbers from a long text) and previously impossible things possible. With LLMs and LangChain you can actually build pleasant assistants that chat with you and understand your intent to a very reasonable degree. The diﬀerence is night and day! LangChain is a tool that lets you build LLM applications with a single command. The book includes the 1st chapter of the book, which explains how to use LangChain to build a chatbot. It also explains how LangChain’s building blocks map to LLM concepts and how they can be combined to build applications. LangChain abstracts away these diﬀerences to enable building applications that are truly independent of a particular provider. With LangChain a chatbot conversation where you use both OpenAI and Anthropic models just works. Long-running LLM applications can be interrupted, resumed, or retried (more on this in Chapter 6) Using LangChain’s OpenAI LLM wrapper to invoke a model prediction using a simple prompt. First in Python: And now in JS: From langchain_openai.llms import OpenAI model = OpenAI(model=' gpt-3.5-turbo-instruct') prompt = 'The sky is' completion = model.invoke(prompt) completionTIP Notice the parameter model passed to OpenAI. This is the most common parameter to conﬁgure when using an LLM. The Chat Model interface enables back and forth conversations between the user and model. The chat models interface makes it easier to conﬁgure and manage conversions in your AI chatbot application. Instead of a single prompt string, chat models make use of diﬀerent types of chat message interfaces associated with each role. LangChain is an open-source chat prompt template library. It can be used to provide dynamic prompts based on the role of the chat message. The prompt template can also be used as a recipe to build multiple static, speciﬁc prompts. We’ll see how we’d feed this into an LLM OpenAI model using LangChain. In Python, the prompt contains instructions in a SystemMessage and two HumanMessages that contain dynamic context and question variables. Y ou can still format the template in the same way , and get back a static prompt that you can pass to a large language model for a prediction output. Plain text outputs are useful, but there may be use cases where you need the LLM to generate a structured output, such as JSON , XML or CSV. Output parsers are classes that help you structure large language model responses. They serve two functions: Providing format instructions, and validating and parsing output. LangChain provides a variety of output parsers for various use cases, including CSV , XML, and more. We’ll see how to combine output Parsers with models and prompts in the next section. Imperative composition is just a fancy name for writing the code you’re used to writing, composing these components into functions and classes. Declarative With LangChain Expression Language (LCEL) is covered in an upcoming section. Table 2-1 summarizes their diﬀerences, and we’ll see each in action next. All of Python or JavaScript LCEL Parallel executionPython: W ith threads or coroutines JavaScript: WIth Promise. allAutomatic Streaming With yield keyword Automatic Async execution. LangChain is a declarative language for composing LangChain components. LangChain compiles LCEL compositions to an optimized execution plan with automatic parallelization, streaming, tracing, and async support. LLM applications are essentially a chain consisting of the large language model, the prompt to guide the model towards a desired output, and an optional output. This is the 2nd chapter of the ﬁnal book. In the previous chapter , you learned about the important building blocks used to create an LLM application using LangChain. In this chapter, you'll learn how to provide external data to your AI chatbot as context. In this chapter and the next, you’ll learn how to overcome this challenge in two steps: 1. Indexing your documents in a way where your application can easily ﬁnd the most relevant ones for each question 2. Retrieving this external data from the index and using it as context for the LLM to generate an accurate output based on your data. Before LLMs, computer scientists were using embeddings to enable full-text searchcapabilities in websites, or to classify emails as spam. In this model, the embedding for I haven’t seen a sunny day in weeks is the sequence of numbers. The model has no awareness of meaning, only of the actual words used. In the next section we’ll see how semanticembeddings address this limitation. An embedding model is an algorithm that takes a piece of text and outputs a numerical representation of its meaning. These are also called dense embeddings. Most embedding models are trained for that purpose alone, following somewhat similar architectures and training processes as LLMs. TIP Diﬀerent models produce diﬄerent numbers, and di�ara�erent sizes of lists. Combining embeddments from di�ricaerent models should always be avoided. The ability to convert sentences into embeddings that capture semantic meaning enables us to get an LLM to answer questions about a large body of text. One eﬀective way to calculate the degree of similarity between two vectors in a multi-dimensional space is called cosine similarity. The narrower the angle, the closer the similarities. LangChain provides document loaders that handle the parsing logic and enable you to “load” data from various sources into a Document class which consists of text and associated metadata. For example, consider a simple .txt ﬁle. Y ou can simply import a LangChain T ext loader class to extract the text. TIP LLMs and Embedding models are designed with a hard limit on the size of input and output text. LangChain provides RecursiveCharacterT extSplitter to split large documents into small, but still meaningful, pieces of text. These LangChain text splitters handle various data formats, including HTML, code, JSON , Markdown, text, and more. LangChain is an open-source text splitters tool. It can be used to split text between different languages. LangChain is available in Python, JS, Markdown, HTML, and many more. You can install LangChain using pip. For more information on LangChain, visit the project's website.  vector store capabilities have recently been extended to P ostgres (a popular open-source relational database) This enables you to use the same database you’re already familiar with, to power both your transactional tables (for instance your users table) as well as your vector search tables. unlock a wide variety of use cases, including scalable applications that utilize AI to answer questions about large documents. LangChain provides an Indexing API to make it easy to keep your documents in sync with your vector store. Let’s see what it looks like to search documents, ﬁrst in Python: db.similarity_search(\"query\", k=4) And in JS: This method will ﬅnd the most relevant documents (which you previously indexed as above) Indexing API. utilizes a class ( RecordManager ) to keep track of document writes into the vector store. When indexing content, hashes are computed for each document, and the following information is stored in RecordManager : The document hash (hash of both page content and metadata) Write time The source id (each document should include information in its metadata to determine the ultimate source of this document)\n"
          ]
        }
      ]
    }
  ]
}